# Introduction

Welcome to your Data Mining code book!!  I have provided information in both R and Python for this module (of course, most emphasis is in R...which means you might need to do some extra work in Python for some of these codes).  Libraries in R that you will need for this module:


```{r library,results = FALSE,warning=FALSE,collapse=TRUE,message=FALSE}
library(datasets)
library(arules)
library(arulesViz)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(TH.data)
library(ISLR2)
library(lattice)
library(stats)
library(rattle)
library(RColorBrewer)
library(caret)
library(ROCR)
library(tidyverse)  
library(cluster)  
library(factoextra) 
library(gridExtra)
library(NbClust)
library(dendextend)
library(class)
library(ClustOfVar)
library(MASS)
library(kableExtra)
library(partykit)
library(dbscan)
library(AmesHousing)
library(reticulate)

use_python("C:\\ProgramData\\Anaconda3\\python.exe")

#library(knitr)

```

Also, be sure to load the following data sets:

```{r data sets}

check=read.table("Q:\\My Drive\\Data Mining\\Data\\Checking.csv",sep=',',header=T)

temp.dat=read.table("Q:\\My Drive\\Data Mining\\Data\\Grocery1.csv",sep=',',header=T)

load("Q:\\My Drive\\Data Mining\\Data\\breast_cancer.Rdata")

ames <- make_ordinal_ames() 

```


## Bootstrapping

Bootstrapping is a nonparametric tool that can be used to estimate variability about a statistic, create confidence intervals and perform hypothesis testing.  The idea is that we use our sample as our population and randomly sample from it (with replacement).  We create a whole bunch of samples this way, estimate the statistic of interest and then can use this information for our inference.  The first example will estimate the median Carbon Dioxide reading from the Mauna Loa station (this data is an R data set and provides monthly data from 1959 to 1997). We will also calculate a 95% confidence interval for the median. \  


```{r bootstrap confidence interval,collapse=TRUE}
plot.co2=cbind.data.frame(co2,seq(as.Date("1959/1/1"),by="month",length.out=468))
colnames(plot.co2)[2]="Date"

### Visualize the data
ggplot(data=plot.co2,aes(x=Date,y=co2))+geom_line()
ggplot(data=plot.co2,aes(x=co2))+geom_histogram()

## Actual median of the data set
act.med=median(co2)
### Initialize the vector to run the bootstrap algorithm
boot.med=vector(length=10000)
for (i in 1:length(boot.med)){
  boot.samp=sample(co2,replace=T)
  boot.med[i]=median(boot.samp)
}
sd(boot.med)
act.med
quantile(boot.med,probs = c(0.025,0.975))
ggplot(data=data.frame(boot.med),aes(x=boot.med))+geom_histogram() + labs(x="Bootstrap Sample",y="Frequency")

```
The second example is exploring if the median price of homes in Iowa with air conditioning is significantly different than the median price of homes in Iowa without air conditioning (this is using the Ames housing data set from summer!).  Recall that distributions of Sale Price were NOT normal (for both with and without air conditioning) and the variability of the distributions were very different.  We are able to test the hypothesis:   


$$H_{0}: Median_{YES} = Median_{NO}$$

$$H_{A}: Median_{YES} \neq Median_{NO}$$

by creating a confidence interval about the difference $ Median_{YES}-Median_{NO} $.

```{r bootstrap hypothesis test}


diff.stat=vector(length=10000)
yes<-ames %>% filter(Central_Air=="Y") %>% pull(Sale_Price)
no<-ames %>% filter(Central_Air=="N") %>% pull(Sale_Price)
for (i in 1:10000){
  yes.vec <-median(sample(yes,length(yes),replace = T))
  no.vec <-median(sample(no,length(no),replace = T))
  diff.stat[i]<- yes.vec-no.vec
}


ggplot(data=data.frame(diff.stat),aes(x=diff.stat))+geom_histogram()

quantile(diff.stat,probs = c(0.005,0.995))

```
Since the confidence interval does NOT include 0, we would conclude that the medians appear to be significantly different.


### Python for Bootstrapping

```{python}
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import statistics
from numpy import random

co2_py = r.co2
plt.hist(co2_py)
plt.show()

# Actual median
print(statistics.median(co2_py))

# Bootstrap sample
n = len(co2_py)
temp = []
for i in range(10000):
  temp.append(statistics.median(random.choice(co2_py,n)))
#temp =statistics.median(random.choice(co2_py,n))

print(statistics.stdev(temp))
print("Lower 95% confidence interval for median",np.quantile(temp,0.025))
print("Upper 95% confidence interval for median",np.quantile(temp,0.975))

plt.hist(temp)
plt.show()

ames_py=r.ames
type(ames_py)
yes=ames_py[ames_py["Central_Air"]=="Y"]
no=ames_py[ames_py["Central_Air"]=="N"]

diff_sale=[]
n=2930
for i in range(10000):
  yes_boot=statistics.median(random.choice(yes["Sale_Price"],n))
  no_boot=statistics.median(random.choice(no["Sale_Price"],n))
  diff_temp=yes_boot-no_boot
  diff_sale.append(diff_temp)
  
plt.hist(diff_sale)
plt.show()
```




## Adjusting p-values

The package p.adjust will do the Bonferroni adjustment, Benjamini & Hochberg and many other adjustments for p-values when you are conducting a lot of tests!  All you need to do is send it a vector of p-values and it will return the adjusted p-values!

```{r adjust p-values,collapse=TRUE}

temp=c(0.001,0.03,0.2,0.4)
#Bonferoni
p.adjust(temp,method="bonferroni")
#Benjamini & Hochberg
p.adjust(temp,method="BH")

```


### Python code for FDR

Please note that the false discover rate command is commented out in this chunk. This is because the stats within scipy needs an earlier version of numpy (which I just updated for the bootstrap algorithm above).


```{python}
#from scipy import stats

p_test = np.array([0.001,0.03,0.2,0.4])
#stats.false_discovery_control(p_test, method='bh')

# Bonferroni
p_test *len(p_test)
```


## Transaction data

A basic concept you will need is how to deal with transactional data.  Trasactional data is usually in the form of a long data set with multiple observation per "ID".  Take for example the following small data set:



```{r echo=FALSE,results='asis'}

kbl(check[1:11,], caption = "Example") %>%
  kable_paper("striped", full_width = F) %>%
  pack_rows("Person 1", 1, 4) %>%
  pack_rows("Person 2", 5, 8) %>%
  pack_rows("Person 3", 9, 11)
```
Notice that there are four transactions for Person 1, four transactions for Person 2 (including a missing value) and three transactions for Person 3. We will need to "roll" these up into one observation per person.  

```{r, roll-up observations}
new.check = check %>% group_by(ID) %>%
  summarise(mean.check=mean(Checking,na.rm=T),std.check=sd(Checking,na.rm=T))

```

It is HIGHLY recommended that you ALWAYS create a flag for missing values (do this BEFORE rolling up). The following code creates the flag for the missing values and then rolls everything up:

```{r roll up with missing}
check$flag=ifelse(is.na(check$Checking),1,0)

new.check = check %>% group_by(ID) %>%
  summarise(mean.check=mean(Checking,na.rm=T),std.check=sd(Checking,na.rm=T),na.check=sum(flag))

```



```{r echo=FALSE,results='asis'}

kbl(new.check, caption = "Rolled up") %>%
  kable_paper("striped", full_width = F) 
  
```




## Association Analysis

Association analysis looks for "associations" among items to provide information on which items happen together more frequently.  This could be used for marketing purposes to figure out which items should be stocked together; or it could be used in the medical profession to see which co-morbidities happen together.  There are many situations in which you are interested in exploring and understanding theses associations!!  

Most often, this data presents itself as transactional data.  For example, if shopper 1 buys 10 items, you will see this in a data set with 10 lines (representing the 10 items bought) but with the same shopper ID value.  Below is a snippet of a small grocery data set.


```{r association, echo=FALSE,results='asis'}

temp.dat=read.table("Q:\\My Drive\\Data Mining\\Data\\Grocery1.csv",sep=',',header=T)
kbl(temp.dat[1:10,], caption = "Small grocery data") %>%
  kable_paper("striped", full_width = F) 


```

As you can see, there is a separate line for each item.  If we want to analyze this data set with the package 'arules', we need the data to be wide (not long).  In other words, one line per ID.  We can create the wide data set in R by using the following line (and then "inspect" it or view it).


```{r transactions}
trans.dat <- as(split(temp.dat$Grocery, temp.dat$ID), "transactions")
inspect(trans.dat)


```

The following code runs the association analysis and stores the results in the object rules.  Once you have this stored object, you can view it and partition it in different ways..


```{r association2}
trans.dat@itemInfo$labels
# Create an item frequency plot for the top 3 items
itemFrequencyPlot(trans.dat,topN=3,type="absolute")
# Get the rules
rules <- apriori(trans.dat, parameter = list(supp = 0.1, conf = 0.001, target="rules"))
rules<-sort(rules, by="confidence", decreasing=TRUE)

inspect(rules[1:4])
oat.rules = apriori(trans.dat, parameter = list(supp=0.001, conf=0.8),appearance = list(default="lhs",rhs="oat packet"))
oat.rules2 = apriori(trans.dat, parameter = list(supp=0.001, conf=0.8),appearance = list(lhs="oat packet", default="rhs"))
plot(rules)
top10rules = head(rules, n = 10, by = "confidence")
plot(top10rules, method = "graph",  engine = "htmlwidget")

```
