# Introduction

Libraries that you will need for this module:


```{r library,results = FALSE,warning=FALSE,collapse=TRUE,message=FALSE}
library(datasets)
library(arules)
library(arulesViz)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(TH.data)
library(ISLR2)
library(lattice)
library(stats)
library(rattle)
library(RColorBrewer)
library(caret)
library(ROCR)
library(tidyverse)  
library(cluster)  
library(factoextra) 
library(gridExtra)
library(NbClust)
library(dendextend)
library(class)
library(ClustOfVar)
library(MASS)
library(kableExtra)
library(partykit)
library(dbscan)
#library(knitr)

```

Also, be sure to load the following data sets:

```{r data sets}

check=read.table("Q:\\My Drive\\Data Mining\\Data\\Checking.csv",sep=',',header=T)

temp.dat=read.table("Q:\\My Drive\\Data Mining\\Data\\Grocery1.csv",sep=',',header=T)

load("Q:\\My Drive\\Data Mining\\Data\\breast_cancer.Rdata")

```


## Bootstrapping

Bootstrapping is a nonparametric tool that can be used to estimate variability about a statistic, create confidence intervals and perform hypothesis testing.  The idea is that we use our sample as our population and randomly sample from it (with replacement).  We create a whole bunch of samples this way, estimate the statistic of interest and then can use this information for our inference.  We will do two examples: one to estimate the variability of a statistic (and create a confidence interval) and a second one to illustrate a hypothesis test.  To illustrate this, we will be using the Carbon Dioxide data set from the Mauna Loa station (monthly data from 1959 to 1997). \  

First, we will find a 95% confidence interval for a median.

```{r bootstrap confidence interval,collapse=TRUE}
plot.co2=cbind.data.frame(co2,seq(as.Date("1959/1/1"),by="month",length.out=468))
colnames(plot.co2)[2]="Date"
ggplot(data=plot.co2,aes(x=Date,y=co2))+geom_line()
ggplot(data=plot.co2,aes(x=co2))+geom_histogram()

act.med=median(co2)
boot.med=vector(length=10000)
for (i in 1:length(boot.med)){
  index.boot=sample(1:length(co2),length(co2),replace=T)
  boot.samp=co2[index.boot]
  boot.med[i]=median(boot.samp)
}
sd(boot.med)
act.med
quantile(boot.med,probs = c(0.025,0.975))
ggplot(data=data.frame(boot.med),aes(x=boot.med))+geom_histogram() + labs(x="Bootstrap Sample",y="Frequency")

```
Now we are going to show how to use the boostrap to develop the null distribution (under the null hypothesis).  We are interested in knowing if the trend in Carbon Dioxide  (let's just use simple linear regression to estimate the trend line) has increased after 1980.  In other words, I want to test the following test: \  




$$H_{0}: \beta_{before1980} = \beta_{after1980}$$

$$H_{A}: \beta_{before1980} < \beta_{after1980}$$

```{r bootstrap hypothesis test}

test.stat=vector(length=10000)
x.before=seq(1,144)
x.after=seq(1,324)
lm.0=lm(co2[1:144]~x.before)
lm.1=lm(co2[145:468]~x.after)
act.test=(lm.0$coef[2]-lm.1$coef[2])/sqrt(diag(vcov(lm.0))[2]+diag(vcov(lm.1))[2])
for (i in 1:length(test.stat)){
  before=sample(1:length(co2),144,replace=T)
  after=sample(1:length(co2),324,replace=T)
  co0=co2[before]
  co1=co2[after]
  lm.0=lm(co0~x.before)
  lm.1=lm(co1~x.after)
  test.stat[i]=(lm.0$coef[2]-lm.1$coef[2])/sqrt(diag(vcov(lm.0))[2]+diag(vcov(lm.1))[2])
}  

ggplot(data=data.frame(test.stat),aes(x=test.stat))+geom_histogram()

p.val=sum(test.stat<act.test)/10000

```
The actual "test statistic" is -13.2, and from the created distribution the number of samples less than -13.2 is 0 (p-value is 0/10000=0).  Based on the very low p-value, we would reject 

## Adjusting p-values

The package p.adjust will do the Bonferroni adjustment, Benjamini & Hochberg and many other adjustments for p-values when you are conducting a lot of tests!  All you need to do is send it a vector of p-values and it will return the adjusted p-values!

```{r adjust p-values,collapse=TRUE}

temp=c(0.001,0.03,0.2,0.4)
#Bonferoni
p.adjust(temp,method="bonferroni")
#Benjamini & Hochberg
p.adjust(temp,method="BH")

```


## Tranaction data

A basic concept you will need is how to deal with transactional data.  Trasactional data is usually in the form of a long data set with multiple observation per "ID".  Take for example the following small data set:



```{r echo=FALSE,results='asis'}

kbl(check[1:11,], caption = "Example") %>%
  kable_paper("striped", full_width = F) %>%
  pack_rows("Person 1", 1, 4) %>%
  pack_rows("Person 2", 5, 8) %>%
  pack_rows("Person 3", 9, 11)
```
Notice that there are four transactions for Person 1, four transactions for Person 2 (including a missing value) and three transactions for Person 3. We will need to "roll" these up into one observation per person.  

```{r, roll-up observations}
new.check = check %>% group_by(ID) %>%
  summarise(mean.check=mean(Checking,na.rm=T),std.check=sd(Checking,na.rm=T))

```

It is HIGHLY recommended that you ALWAYS create a flag for missing values (do this BEFORE rolling up). The following code creates the flag for the missing values and then rolls everything up:

```{r roll up with missing}
check$flag=ifelse(is.na(check$Checking),1,0)

new.check = check %>% group_by(ID) %>%
  summarise(mean.check=mean(Checking,na.rm=T),std.check=sd(Checking,na.rm=T),na.check=sum(flag))

```



```{r echo=FALSE,results='asis'}

kbl(new.check, caption = "Rolled up") %>%
  kable_paper("striped", full_width = F) 
  
```




## Association Analysis

Association analysis looks for "associations" among items to provide information on which items happen together more frequently.  This could be used for marketing purposes to figure out which items should be stocked together; or it could be used in the medical profession to see which co-morbidities happen together.  There are many situations in which you are interested in exploring and understanding theses associations!!  

Most often, this data presents itself as transactional data.  For example, if shopper 1 buys 10 items, you will see this in a dataset with 10 lines (representing the 10 items bought) but with the same shopper ID value.  Below is a snippet of a small grocery data set.


```{r association, echo=FALSE,results='asis'}

temp.dat=read.table("Q:\\My Drive\\Data Mining\\Data\\Grocery1.csv",sep=',',header=T)
kbl(temp.dat[1:10,], caption = "Small grocery data") %>%
  kable_paper("striped", full_width = F) 


```

As you can see, there is a separate line for each item.  If we want to analyze this data set with the package 'arules', we need the data to be wide (not long).  In other words, one line per ID.  We can create the wide data set in R by using the following line (and then "inspect" it or view it).


```{r transactions}
trans.dat <- as(split(temp.dat$Grocery, temp.dat$ID), "transactions")
inspect(trans.dat)


```

The following code runs the association analysis and stores the results in the object rules.  Once you have this stored object, you can view it and partition it in different ways..


```{r association2}
trans.dat@itemInfo$labels
# Create an item frequency plot for the top 3 items
itemFrequencyPlot(trans.dat,topN=3,type="absolute")
# Get the rules
rules <- apriori(trans.dat, parameter = list(supp = 0.1, conf = 0.001, target="rules"))
rules<-sort(rules, by="confidence", decreasing=TRUE)

inspect(rules[1:4])
oat.rules = apriori(trans.dat, parameter = list(supp=0.001, conf=0.8),appearance = list(default="lhs",rhs="oat packet"))
oat.rules2 = apriori(trans.dat, parameter = list(supp=0.001, conf=0.8),appearance = list(lhs="oat packet", default="rhs"))
plot(rules)
top10rules = head(rules, n = 10, by = "confidence")
plot(top10rules, method = "graph",  engine = "htmlwidget")

```
