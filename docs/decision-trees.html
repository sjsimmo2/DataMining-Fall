<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Decision Trees | Data Mining</title>
  <meta name="description" content="Chapter 3 Decision Trees | Data Mining" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Decision Trees | Data Mining" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Decision Trees | Data Mining" />
  
  
  

<meta name="author" content="Dr.Â Susan Simmons" />


<meta name="date" content="2023-10-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="clustering.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DataMining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Data Mining</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#bootstrapping"><i class="fa fa-check"></i><b>2.1</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction.html"><a href="introduction.html#python-for-bootstrapping"><i class="fa fa-check"></i><b>2.1.1</b> Python for Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#adjusting-p-values"><i class="fa fa-check"></i><b>2.2</b> Adjusting p-values</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction.html"><a href="introduction.html#python-code-for-fdr"><i class="fa fa-check"></i><b>2.2.1</b> Python code for FDR</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#transaction-data"><i class="fa fa-check"></i><b>2.3</b> Transaction data</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#association-analysis"><i class="fa fa-check"></i><b>2.4</b> Association Analysis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#python-code-for-association-analysis"><i class="fa fa-check"></i><b>2.4.1</b> Python code for Association Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>3</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-trees"><i class="fa fa-check"></i><b>3.1</b> Classification Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="decision-trees.html"><a href="decision-trees.html#python-classification-trees"><i class="fa fa-check"></i><b>3.1.1</b> Python Classification Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>3.2</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees-in-python"><i class="fa fa-check"></i><b>3.2.1</b> Regression trees in Python</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="decision-trees.html"><a href="decision-trees.html#recursive-partitioning-with-partykit"><i class="fa fa-check"></i><b>3.3</b> Recursive partitioning with partykit</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="decision-trees.html"><a href="decision-trees.html#python-for-conditional-inference-decision-trees"><i class="fa fa-check"></i><b>3.3.1</b> Python for Conditional Inference Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="decision-trees.html"><a href="decision-trees.html#model-reliance"><i class="fa fa-check"></i><b>3.4</b> Model Reliance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>4</b> Clustering</a>
<ul>
<li class="chapter" data-level="4.1" data-path="clustering.html"><a href="clustering.html#kmeans"><i class="fa fa-check"></i><b>4.1</b> Kmeans</a></li>
<li class="chapter" data-level="4.2" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="4.3" data-path="clustering.html"><a href="clustering.html#variable-clustering"><i class="fa fa-check"></i><b>4.3</b> Variable Clustering</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="other-topics.html"><a href="other-topics.html"><i class="fa fa-check"></i><b>5</b> Other Topics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="other-topics.html"><a href="other-topics.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.1</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2" data-path="other-topics.html"><a href="other-topics.html#mds"><i class="fa fa-check"></i><b>5.2</b> MDS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Decision Trees<a href="decision-trees.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We will now discuss decision tree. Two of the most popular algorithms in R is rpart and partykit. We will fist focus on classification trees (response variable is categorical). The data in the code below uses the breast cancer data set from the UCI repository.</p>
<div id="classification-trees" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Classification Trees<a href="decision-trees.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="decision-trees.html#cb1-1" tabindex="-1"></a><span class="do">###Classification</span></span>
<span id="cb1-2"><a href="decision-trees.html#cb1-2" tabindex="-1"></a><span class="do">### Get training and test data</span></span>
<span id="cb1-3"><a href="decision-trees.html#cb1-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7515</span>)</span>
<span id="cb1-4"><a href="decision-trees.html#cb1-4" tabindex="-1"></a>perm<span class="ot">=</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">699</span>)</span>
<span id="cb1-5"><a href="decision-trees.html#cb1-5" tabindex="-1"></a>BC_randomOrder<span class="ot">=</span>BCdata[perm,]</span>
<span id="cb1-6"><a href="decision-trees.html#cb1-6" tabindex="-1"></a>train <span class="ot">=</span> BC_randomOrder[<span class="dv">1</span><span class="sc">:</span><span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="dv">699</span>),]</span>
<span id="cb1-7"><a href="decision-trees.html#cb1-7" tabindex="-1"></a>test <span class="ot">=</span> BC_randomOrder[(<span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="dv">699</span>)<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">699</span>,]</span>
<span id="cb1-8"><a href="decision-trees.html#cb1-8" tabindex="-1"></a>BC.tree <span class="ot">=</span> <span class="fu">rpart</span>(Target <span class="sc">~</span> . <span class="sc">-</span> ID, <span class="at">data=</span>train, <span class="at">method=</span><span class="st">&#39;class&#39;</span>,</span>
<span id="cb1-9"><a href="decision-trees.html#cb1-9" tabindex="-1"></a> <span class="at">parms =</span> <span class="fu">list</span>(<span class="at">split=</span><span class="st">&#39;gini&#39;</span>)) <span class="do">## or &#39;information&#39;</span></span>
<span id="cb1-10"><a href="decision-trees.html#cb1-10" tabindex="-1"></a><span class="fu">summary</span>(BC.tree)</span></code></pre></div>
<pre><code>## Call:
## rpart(formula = Target ~ . - ID, data = train, method = &quot;class&quot;, 
##     parms = list(split = &quot;gini&quot;))
##   n= 524 
## 
##           CP nsplit  rel error    xerror       xstd
## 1 0.79781421      0 1.00000000 1.0000000 0.05963291
## 2 0.07650273      1 0.20218579 0.2622951 0.03608339
## 3 0.01639344      2 0.12568306 0.1693989 0.02951125
## 4 0.01000000      4 0.09289617 0.1639344 0.02906079
## 
## Variable importance
##       Size      Shape     Normal  Chromatin Epithelial     Margin         CT 
##         22         18         16         15         14         13          2 
##       Bare 
##          1 
## 
## Node number 1: 524 observations,    complexity param=0.7978142
##   predicted class=0  expected loss=0.3492366  P(node) =1
##     class counts:   341   183
##    probabilities: 0.651 0.349 
##   left son=2 (360 obs) right son=3 (164 obs)
##   Primary splits:
##       Size       &lt; 3.5 to the left,  improve=169.5227, (0 missing)
##       Shape      &lt; 3.5 to the left,  improve=164.4060, (0 missing)
##       Bare       &lt; 2.5 to the left,  improve=152.7409, (10 missing)
##       Epithelial &lt; 2.5 to the left,  improve=148.5429, (0 missing)
##       Normal     &lt; 2.5 to the left,  improve=146.2118, (0 missing)
##   Surrogate splits:
##       Shape      &lt; 3.5 to the left,  agree=0.933, adj=0.787, (0 split)
##       Epithelial &lt; 3.5 to the left,  agree=0.889, adj=0.646, (0 split)
##       Chromatin  &lt; 3.5 to the left,  agree=0.889, adj=0.646, (0 split)
##       Normal     &lt; 2.5 to the left,  agree=0.874, adj=0.598, (0 split)
##       Margin     &lt; 3.5 to the left,  agree=0.868, adj=0.579, (0 split)
## 
## Node number 2: 360 observations,    complexity param=0.07650273
##   predicted class=0  expected loss=0.07777778  P(node) =0.6870229
##     class counts:   332    28
##    probabilities: 0.922 0.078 
##   left son=4 (334 obs) right son=5 (26 obs)
##   Primary splits:
##       Normal    &lt; 3.5 to the left,  improve=26.79691, (0 missing)
##       Bare      &lt; 6   to the left,  improve=24.86222, (7 missing)
##       CT        &lt; 6.5 to the left,  improve=23.10046, (0 missing)
##       Shape     &lt; 3.5 to the left,  improve=16.21263, (0 missing)
##       Chromatin &lt; 3.5 to the left,  improve=15.47227, (0 missing)
##   Surrogate splits:
##       Shape      &lt; 4.5 to the left,  agree=0.944, adj=0.231, (0 split)
##       CT         &lt; 6.5 to the left,  agree=0.942, adj=0.192, (0 split)
##       Chromatin  &lt; 3.5 to the left,  agree=0.942, adj=0.192, (0 split)
##       Margin     &lt; 3.5 to the left,  agree=0.939, adj=0.154, (0 split)
##       Epithelial &lt; 4.5 to the left,  agree=0.936, adj=0.115, (0 split)
## 
## Node number 3: 164 observations
##   predicted class=1  expected loss=0.05487805  P(node) =0.3129771
##     class counts:     9   155
##    probabilities: 0.055 0.945 
## 
## Node number 4: 334 observations,    complexity param=0.01639344
##   predicted class=0  expected loss=0.0239521  P(node) =0.6374046
##     class counts:   326     8
##    probabilities: 0.976 0.024 
##   left son=8 (308 obs) right son=9 (26 obs)
##   Primary splits:
##       Bare       &lt; 2.5 to the left,  improve=4.531640, (7 missing)
##       Epithelial &lt; 2.5 to the left,  improve=2.685889, (0 missing)
##       Shape      &lt; 2.5 to the left,  improve=2.202471, (0 missing)
##       CT         &lt; 5.5 to the left,  improve=1.850413, (0 missing)
##       Size       &lt; 1.5 to the left,  improve=1.583809, (0 missing)
##   Surrogate splits:
##       CT         &lt; 6.5 to the left,  agree=0.927, adj=0.077, (7 split)
##       Margin     &lt; 7.5 to the left,  agree=0.927, adj=0.077, (0 split)
##       Epithelial &lt; 5.5 to the left,  agree=0.927, adj=0.077, (0 split)
## 
## Node number 5: 26 observations
##   predicted class=1  expected loss=0.2307692  P(node) =0.04961832
##     class counts:     6    20
##    probabilities: 0.231 0.769 
## 
## Node number 8: 308 observations
##   predicted class=0  expected loss=0  P(node) =0.5877863
##     class counts:   308     0
##    probabilities: 1.000 0.000 
## 
## Node number 9: 26 observations,    complexity param=0.01639344
##   predicted class=0  expected loss=0.3076923  P(node) =0.04961832
##     class counts:    18     8
##    probabilities: 0.692 0.308 
##   left son=18 (16 obs) right son=19 (10 obs)
##   Primary splits:
##       CT         &lt; 3.5 to the left,  improve=7.876923, (0 missing)
##       Size       &lt; 1.5 to the left,  improve=7.438034, (0 missing)
##       Shape      &lt; 2.5 to the left,  improve=7.438034, (0 missing)
##       Chromatin  &lt; 1.5 to the left,  improve=4.923077, (0 missing)
##       Epithelial &lt; 2.5 to the left,  improve=4.119347, (0 missing)
##   Surrogate splits:
##       Size       &lt; 1.5 to the left,  agree=0.923, adj=0.8, (0 split)
##       Shape      &lt; 1.5 to the left,  agree=0.923, adj=0.8, (0 split)
##       Epithelial &lt; 2.5 to the left,  agree=0.808, adj=0.5, (0 split)
##       Chromatin  &lt; 1.5 to the left,  agree=0.808, adj=0.5, (0 split)
##       Margin     &lt; 2.5 to the left,  agree=0.769, adj=0.4, (0 split)
## 
## Node number 18: 16 observations
##   predicted class=0  expected loss=0  P(node) =0.03053435
##     class counts:    16     0
##    probabilities: 1.000 0.000 
## 
## Node number 19: 10 observations
##   predicted class=1  expected loss=0.2  P(node) =0.01908397
##     class counts:     2     8
##    probabilities: 0.200 0.800</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="decision-trees.html#cb3-1" tabindex="-1"></a><span class="fu">print</span>(BC.tree)</span></code></pre></div>
<pre><code>## n= 524 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 524 183 0 (0.65076336 0.34923664)  
##    2) Size&lt; 3.5 360  28 0 (0.92222222 0.07777778)  
##      4) Normal&lt; 3.5 334   8 0 (0.97604790 0.02395210)  
##        8) Bare&lt; 2.5 308   0 0 (1.00000000 0.00000000) *
##        9) Bare&gt;=2.5 26   8 0 (0.69230769 0.30769231)  
##         18) CT&lt; 3.5 16   0 0 (1.00000000 0.00000000) *
##         19) CT&gt;=3.5 10   2 1 (0.20000000 0.80000000) *
##      5) Normal&gt;=3.5 26   6 1 (0.23076923 0.76923077) *
##    3) Size&gt;=3.5 164   9 1 (0.05487805 0.94512195) *</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="decision-trees.html#cb5-1" tabindex="-1"></a>BC.tree<span class="sc">$</span>variable.importance</span></code></pre></div>
<pre><code>##       Size      Shape     Normal  Chromatin Epithelial     Margin         CT 
##  175.82429  145.82955  128.09709  118.66130  116.94858  105.82111   13.37876 
##       Bare 
##    4.53164</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="decision-trees.html#cb7-1" tabindex="-1"></a>varimp.data<span class="ot">=</span><span class="fu">data.frame</span>(BC.tree<span class="sc">$</span>variable.importance)</span>
<span id="cb7-2"><a href="decision-trees.html#cb7-2" tabindex="-1"></a>varimp.data<span class="sc">$</span>names<span class="ot">=</span><span class="fu">as.character</span>(<span class="fu">rownames</span>(varimp.data))</span>
<span id="cb7-3"><a href="decision-trees.html#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="decision-trees.html#cb7-4" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>varimp.data,<span class="fu">aes</span>(<span class="at">x=</span>names,<span class="at">y=</span>BC.tree.variable.importance))<span class="sc">+</span><span class="fu">geom_bar</span>(<span class="at">stat=</span><span class="st">&quot;identity&quot;</span>)<span class="sc">+</span><span class="fu">coord_flip</span>()<span class="sc">+</span><span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Variable Name&quot;</span>,<span class="at">y=</span><span class="st">&quot;Variable Importance&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Classification%20trees-1.png" width="672" /></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="decision-trees.html#cb8-1" tabindex="-1"></a>tscores <span class="ot">=</span> <span class="fu">predict</span>(BC.tree,<span class="at">type=</span><span class="st">&#39;class&#39;</span>)</span>
<span id="cb8-2"><a href="decision-trees.html#cb8-2" tabindex="-1"></a>scores <span class="ot">=</span> <span class="fu">predict</span>(BC.tree, test, <span class="at">type=</span><span class="st">&#39;class&#39;</span>)</span>
<span id="cb8-3"><a href="decision-trees.html#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="decision-trees.html#cb8-4" tabindex="-1"></a><span class="do">##Training misclassification rate:</span></span>
<span id="cb8-5"><a href="decision-trees.html#cb8-5" tabindex="-1"></a><span class="fu">sum</span>(tscores<span class="sc">!=</span>train<span class="sc">$</span>Target)<span class="sc">/</span><span class="fu">nrow</span>(train)</span></code></pre></div>
<pre><code>## [1] 0.03244275</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="decision-trees.html#cb10-1" tabindex="-1"></a><span class="do">### Test data:</span></span>
<span id="cb10-2"><a href="decision-trees.html#cb10-2" tabindex="-1"></a><span class="fu">sum</span>(scores<span class="sc">!=</span>test<span class="sc">$</span>Target)<span class="sc">/</span><span class="fu">nrow</span>(test)</span></code></pre></div>
<pre><code>## [1] 0.05714286</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="decision-trees.html#cb12-1" tabindex="-1"></a><span class="fu">rpart.plot</span>(BC.tree)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Classification%20trees-2.png" width="672" /></p>
<div id="python-classification-trees" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Python Classification Trees<a href="decision-trees.html#python-classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that Python is NOT able to handle categorical variables in its basic tree classifier. This means that if you do have categorical variables, you will need to one-hot encode these variables before using the tree classifier. For this data set, all variables are recorded as integer, so this is an exercise we do not need to worry about (but please do keep this in mind!!).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="decision-trees.html#cb13-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="decision-trees.html#cb13-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-3"><a href="decision-trees.html#cb13-3" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb13-4"><a href="decision-trees.html#cb13-4" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-5"><a href="decision-trees.html#cb13-5" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb13-6"><a href="decision-trees.html#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="decision-trees.html#cb13-7" tabindex="-1"></a>bcdata_py<span class="op">=</span>r.BCdata</span>
<span id="cb13-8"><a href="decision-trees.html#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="decision-trees.html#cb13-9" tabindex="-1"></a>X<span class="op">=</span>bcdata_py.iloc[:,<span class="dv">1</span>:<span class="dv">10</span>]</span>
<span id="cb13-10"><a href="decision-trees.html#cb13-10" tabindex="-1"></a>y<span class="op">=</span>bcdata_py[<span class="st">&#39;Target&#39;</span>]</span>
<span id="cb13-11"><a href="decision-trees.html#cb13-11" tabindex="-1"></a></span>
<span id="cb13-12"><a href="decision-trees.html#cb13-12" tabindex="-1"></a>X_train, X_test,y_train, y_test <span class="op">=</span> train_test_split(X,y ,</span>
<span id="cb13-13"><a href="decision-trees.html#cb13-13" tabindex="-1"></a>                                   random_state<span class="op">=</span><span class="dv">49865</span>, </span>
<span id="cb13-14"><a href="decision-trees.html#cb13-14" tabindex="-1"></a>                                   test_size<span class="op">=</span><span class="fl">0.25</span>, </span>
<span id="cb13-15"><a href="decision-trees.html#cb13-15" tabindex="-1"></a>                                   shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-16"><a href="decision-trees.html#cb13-16" tabindex="-1"></a></span>
<span id="cb13-17"><a href="decision-trees.html#cb13-17" tabindex="-1"></a><span class="co">## Now fit the tree</span></span>
<span id="cb13-18"><a href="decision-trees.html#cb13-18" tabindex="-1"></a>class_tree <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">&#39;entropy&#39;</span>, max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-19"><a href="decision-trees.html#cb13-19" tabindex="-1"></a>class_tree <span class="op">=</span> class_tree.fit(X_train,y_train)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="decision-trees.html#cb15-1" tabindex="-1"></a>y_pred <span class="op">=</span> class_tree.predict(X_test)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="decision-trees.html#cb17-1" tabindex="-1"></a>conf <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb17-2"><a href="decision-trees.html#cb17-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Confusion matrix</span><span class="ch">\n\n</span><span class="st">&#39;</span>, conf)</span></code></pre></div>
<pre><code>## Confusion matrix
## 
##  [[110  10]
##  [  3  52]]</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="decision-trees.html#cb19-1" tabindex="-1"></a>tree.plot_tree(class_tree)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="decision-trees.html#cb20-1" tabindex="-1"></a>class_tree2 <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">&#39;gini&#39;</span>, max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb20-2"><a href="decision-trees.html#cb20-2" tabindex="-1"></a>class_tree2 <span class="op">=</span> class_tree2.fit(X_train,y_train)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="decision-trees.html#cb22-1" tabindex="-1"></a>y_pred <span class="op">=</span> class_tree2.predict(X_test)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="decision-trees.html#cb24-1" tabindex="-1"></a>conf <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb24-2"><a href="decision-trees.html#cb24-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Confusion matrix</span><span class="ch">\n\n</span><span class="st">&#39;</span>, conf)</span></code></pre></div>
<pre><code>## Confusion matrix
## 
##  [[110  10]
##  [  4  51]]</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="decision-trees.html#cb26-1" tabindex="-1"></a>tree.plot_tree(class_tree2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
</div>
</div>
<div id="regression-trees" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Regression Trees<a href="decision-trees.html#regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The code below illustrates regression trees on a version of the bodyfat data set.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="decision-trees.html#cb27-1" tabindex="-1"></a><span class="do">###Regression</span></span>
<span id="cb27-2"><a href="decision-trees.html#cb27-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">13172</span>) <span class="co"># Set Seed so that same sample can be reproduced in future also</span></span>
<span id="cb27-3"><a href="decision-trees.html#cb27-3" tabindex="-1"></a><span class="co"># Now Selecting 75% of data as sample from total &#39;n&#39; rows of the data  </span></span>
<span id="cb27-4"><a href="decision-trees.html#cb27-4" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="at">n =</span> <span class="fu">nrow</span>(bodyfat), <span class="at">size =</span> <span class="fu">floor</span>(.<span class="dv">75</span><span class="sc">*</span><span class="fu">nrow</span>(bodyfat)), <span class="at">replace =</span> F)</span>
<span id="cb27-5"><a href="decision-trees.html#cb27-5" tabindex="-1"></a>train <span class="ot">&lt;-</span> bodyfat[sample, ]</span>
<span id="cb27-6"><a href="decision-trees.html#cb27-6" tabindex="-1"></a>test  <span class="ot">&lt;-</span> bodyfat[<span class="sc">-</span>sample, ] </span>
<span id="cb27-7"><a href="decision-trees.html#cb27-7" tabindex="-1"></a></span>
<span id="cb27-8"><a href="decision-trees.html#cb27-8" tabindex="-1"></a>body_model<span class="ot">&lt;-</span><span class="fu">rpart</span>(DEXfat <span class="sc">~</span> age <span class="sc">+</span> waistcirc <span class="sc">+</span> hipcirc <span class="sc">+</span></span>
<span id="cb27-9"><a href="decision-trees.html#cb27-9" tabindex="-1"></a>  elbowbreadth <span class="sc">+</span> kneebreadth, <span class="at">data =</span> train,</span>
<span id="cb27-10"><a href="decision-trees.html#cb27-10" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">10</span>))</span>
<span id="cb27-11"><a href="decision-trees.html#cb27-11" tabindex="-1"></a> <span class="fu">summary</span>(body_model)</span>
<span id="cb27-12"><a href="decision-trees.html#cb27-12" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb27-13"><a href="decision-trees.html#cb27-13" tabindex="-1"></a><span class="do">## rpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + </span></span>
<span id="cb27-14"><a href="decision-trees.html#cb27-14" tabindex="-1"></a><span class="do">##     kneebreadth, data = train, control = rpart.control(minsplit = 10))</span></span>
<span id="cb27-15"><a href="decision-trees.html#cb27-15" tabindex="-1"></a><span class="do">##   n= 53 </span></span>
<span id="cb27-16"><a href="decision-trees.html#cb27-16" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-17"><a href="decision-trees.html#cb27-17" tabindex="-1"></a><span class="do">##           CP nsplit  rel error    xerror       xstd</span></span>
<span id="cb27-18"><a href="decision-trees.html#cb27-18" tabindex="-1"></a><span class="do">## 1 0.67492210      0 1.00000000 1.0387549 0.19298200</span></span>
<span id="cb27-19"><a href="decision-trees.html#cb27-19" tabindex="-1"></a><span class="do">## 2 0.12397892      1 0.32507790 0.3674702 0.09713363</span></span>
<span id="cb27-20"><a href="decision-trees.html#cb27-20" tabindex="-1"></a><span class="do">## 3 0.06337174      2 0.20109898 0.3207674 0.07055438</span></span>
<span id="cb27-21"><a href="decision-trees.html#cb27-21" tabindex="-1"></a><span class="do">## 4 0.04539485      3 0.13772723 0.3298414 0.07030078</span></span>
<span id="cb27-22"><a href="decision-trees.html#cb27-22" tabindex="-1"></a><span class="do">## 5 0.01311181      4 0.09233238 0.2635804 0.07215928</span></span>
<span id="cb27-23"><a href="decision-trees.html#cb27-23" tabindex="-1"></a><span class="do">## 6 0.01000000      5 0.07922057 0.2502471 0.06967234</span></span>
<span id="cb27-24"><a href="decision-trees.html#cb27-24" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-25"><a href="decision-trees.html#cb27-25" tabindex="-1"></a><span class="do">## Variable importance</span></span>
<span id="cb27-26"><a href="decision-trees.html#cb27-26" tabindex="-1"></a><span class="do">##    waistcirc      hipcirc  kneebreadth elbowbreadth          age </span></span>
<span id="cb27-27"><a href="decision-trees.html#cb27-27" tabindex="-1"></a><span class="do">##           30           25           25           11            9 </span></span>
<span id="cb27-28"><a href="decision-trees.html#cb27-28" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-29"><a href="decision-trees.html#cb27-29" tabindex="-1"></a><span class="do">## Node number 1: 53 observations,    complexity param=0.6749221</span></span>
<span id="cb27-30"><a href="decision-trees.html#cb27-30" tabindex="-1"></a><span class="do">##   mean=31.98226, MSE=126.5508 </span></span>
<span id="cb27-31"><a href="decision-trees.html#cb27-31" tabindex="-1"></a><span class="do">##   left son=2 (25 obs) right son=3 (28 obs)</span></span>
<span id="cb27-32"><a href="decision-trees.html#cb27-32" tabindex="-1"></a><span class="do">##   Primary splits:</span></span>
<span id="cb27-33"><a href="decision-trees.html#cb27-33" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 86.5   to the left,  improve=0.6749221, (0 missing)</span></span>
<span id="cb27-34"><a href="decision-trees.html#cb27-34" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 109.25 to the left,  improve=0.6400649, (0 missing)</span></span>
<span id="cb27-35"><a href="decision-trees.html#cb27-35" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 9.35   to the left,  improve=0.4913650, (0 missing)</span></span>
<span id="cb27-36"><a href="decision-trees.html#cb27-36" tabindex="-1"></a><span class="do">##       age          &lt; 37     to the left,  improve=0.1897384, (0 missing)</span></span>
<span id="cb27-37"><a href="decision-trees.html#cb27-37" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.55   to the left,  improve=0.1514361, (0 missing)</span></span>
<span id="cb27-38"><a href="decision-trees.html#cb27-38" tabindex="-1"></a><span class="do">##   Surrogate splits:</span></span>
<span id="cb27-39"><a href="decision-trees.html#cb27-39" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 107.85 to the left,  agree=0.887, adj=0.76, (0 split)</span></span>
<span id="cb27-40"><a href="decision-trees.html#cb27-40" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 9.35   to the left,  agree=0.849, adj=0.68, (0 split)</span></span>
<span id="cb27-41"><a href="decision-trees.html#cb27-41" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.55   to the left,  agree=0.679, adj=0.32, (0 split)</span></span>
<span id="cb27-42"><a href="decision-trees.html#cb27-42" tabindex="-1"></a><span class="do">##       age          &lt; 37     to the left,  agree=0.660, adj=0.28, (0 split)</span></span>
<span id="cb27-43"><a href="decision-trees.html#cb27-43" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-44"><a href="decision-trees.html#cb27-44" tabindex="-1"></a><span class="do">## Node number 2: 25 observations,    complexity param=0.04539485</span></span>
<span id="cb27-45"><a href="decision-trees.html#cb27-45" tabindex="-1"></a><span class="do">##   mean=22.2016, MSE=24.60555 </span></span>
<span id="cb27-46"><a href="decision-trees.html#cb27-46" tabindex="-1"></a><span class="do">##   left son=4 (11 obs) right son=5 (14 obs)</span></span>
<span id="cb27-47"><a href="decision-trees.html#cb27-47" tabindex="-1"></a><span class="do">##   Primary splits:</span></span>
<span id="cb27-48"><a href="decision-trees.html#cb27-48" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 96.5   to the left,  improve=0.49496490, (0 missing)</span></span>
<span id="cb27-49"><a href="decision-trees.html#cb27-49" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 75.15  to the left,  improve=0.43069200, (0 missing)</span></span>
<span id="cb27-50"><a href="decision-trees.html#cb27-50" tabindex="-1"></a><span class="do">##       age          &lt; 31.5   to the left,  improve=0.33752490, (0 missing)</span></span>
<span id="cb27-51"><a href="decision-trees.html#cb27-51" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 8.55   to the left,  improve=0.24410760, (0 missing)</span></span>
<span id="cb27-52"><a href="decision-trees.html#cb27-52" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.65   to the left,  improve=0.08933998, (0 missing)</span></span>
<span id="cb27-53"><a href="decision-trees.html#cb27-53" tabindex="-1"></a><span class="do">##   Surrogate splits:</span></span>
<span id="cb27-54"><a href="decision-trees.html#cb27-54" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 72     to the left,  agree=0.76, adj=0.455, (0 split)</span></span>
<span id="cb27-55"><a href="decision-trees.html#cb27-55" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 8.25   to the left,  agree=0.76, adj=0.455, (0 split)</span></span>
<span id="cb27-56"><a href="decision-trees.html#cb27-56" tabindex="-1"></a><span class="do">##       age          &lt; 31.5   to the left,  agree=0.72, adj=0.364, (0 split)</span></span>
<span id="cb27-57"><a href="decision-trees.html#cb27-57" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.45   to the left,  agree=0.64, adj=0.182, (0 split)</span></span>
<span id="cb27-58"><a href="decision-trees.html#cb27-58" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-59"><a href="decision-trees.html#cb27-59" tabindex="-1"></a><span class="do">## Node number 3: 28 observations,    complexity param=0.1239789</span></span>
<span id="cb27-60"><a href="decision-trees.html#cb27-60" tabindex="-1"></a><span class="do">##   mean=40.715, MSE=55.90078 </span></span>
<span id="cb27-61"><a href="decision-trees.html#cb27-61" tabindex="-1"></a><span class="do">##   left son=6 (25 obs) right son=7 (3 obs)</span></span>
<span id="cb27-62"><a href="decision-trees.html#cb27-62" tabindex="-1"></a><span class="do">##   Primary splits:</span></span>
<span id="cb27-63"><a href="decision-trees.html#cb27-63" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 11.15  to the left,  improve=0.53126700, (0 missing)</span></span>
<span id="cb27-64"><a href="decision-trees.html#cb27-64" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 109.9  to the left,  improve=0.49501800, (0 missing)</span></span>
<span id="cb27-65"><a href="decision-trees.html#cb27-65" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 106    to the left,  improve=0.48547170, (0 missing)</span></span>
<span id="cb27-66"><a href="decision-trees.html#cb27-66" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.35   to the left,  improve=0.18322950, (0 missing)</span></span>
<span id="cb27-67"><a href="decision-trees.html#cb27-67" tabindex="-1"></a><span class="do">##       age          &lt; 64     to the left,  improve=0.06033762, (0 missing)</span></span>
<span id="cb27-68"><a href="decision-trees.html#cb27-68" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-69"><a href="decision-trees.html#cb27-69" tabindex="-1"></a><span class="do">## Node number 4: 11 observations,    complexity param=0.01311181</span></span>
<span id="cb27-70"><a href="decision-trees.html#cb27-70" tabindex="-1"></a><span class="do">##   mean=18.26455, MSE=16.37561 </span></span>
<span id="cb27-71"><a href="decision-trees.html#cb27-71" tabindex="-1"></a><span class="do">##   left son=8 (7 obs) right son=9 (4 obs)</span></span>
<span id="cb27-72"><a href="decision-trees.html#cb27-72" tabindex="-1"></a><span class="do">##   Primary splits:</span></span>
<span id="cb27-73"><a href="decision-trees.html#cb27-73" tabindex="-1"></a><span class="do">##       age          &lt; 56.5   to the left,  improve=0.48821750, (0 missing)</span></span>
<span id="cb27-74"><a href="decision-trees.html#cb27-74" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 77.5   to the left,  improve=0.37437570, (0 missing)</span></span>
<span id="cb27-75"><a href="decision-trees.html#cb27-75" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 94.1   to the left,  improve=0.26063300, (0 missing)</span></span>
<span id="cb27-76"><a href="decision-trees.html#cb27-76" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 8.55   to the left,  improve=0.02392090, (0 missing)</span></span>
<span id="cb27-77"><a href="decision-trees.html#cb27-77" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.15   to the left,  improve=0.02295546, (0 missing)</span></span>
<span id="cb27-78"><a href="decision-trees.html#cb27-78" tabindex="-1"></a><span class="do">##   Surrogate splits:</span></span>
<span id="cb27-79"><a href="decision-trees.html#cb27-79" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 77.5   to the left,  agree=0.818, adj=0.50, (0 split)</span></span>
<span id="cb27-80"><a href="decision-trees.html#cb27-80" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.55   to the left,  agree=0.818, adj=0.50, (0 split)</span></span>
<span id="cb27-81"><a href="decision-trees.html#cb27-81" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 91.5   to the right, agree=0.727, adj=0.25, (0 split)</span></span>
<span id="cb27-82"><a href="decision-trees.html#cb27-82" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-83"><a href="decision-trees.html#cb27-83" tabindex="-1"></a><span class="do">## Node number 5: 14 observations</span></span>
<span id="cb27-84"><a href="decision-trees.html#cb27-84" tabindex="-1"></a><span class="do">##   mean=25.295, MSE=9.323925 </span></span>
<span id="cb27-85"><a href="decision-trees.html#cb27-85" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-86"><a href="decision-trees.html#cb27-86" tabindex="-1"></a><span class="do">## Node number 6: 25 observations,    complexity param=0.06337174</span></span>
<span id="cb27-87"><a href="decision-trees.html#cb27-87" tabindex="-1"></a><span class="do">##   mean=38.8272, MSE=23.49564 </span></span>
<span id="cb27-88"><a href="decision-trees.html#cb27-88" tabindex="-1"></a><span class="do">##   left son=12 (11 obs) right son=13 (14 obs)</span></span>
<span id="cb27-89"><a href="decision-trees.html#cb27-89" tabindex="-1"></a><span class="do">##   Primary splits:</span></span>
<span id="cb27-90"><a href="decision-trees.html#cb27-90" tabindex="-1"></a><span class="do">##       hipcirc      &lt; 109.9  to the left,  improve=0.72361790, (0 missing)</span></span>
<span id="cb27-91"><a href="decision-trees.html#cb27-91" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 98.75  to the left,  improve=0.46229770, (0 missing)</span></span>
<span id="cb27-92"><a href="decision-trees.html#cb27-92" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.35   to the left,  improve=0.24168590, (0 missing)</span></span>
<span id="cb27-93"><a href="decision-trees.html#cb27-93" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 9.9    to the left,  improve=0.22697980, (0 missing)</span></span>
<span id="cb27-94"><a href="decision-trees.html#cb27-94" tabindex="-1"></a><span class="do">##       age          &lt; 60.5   to the left,  improve=0.03275597, (0 missing)</span></span>
<span id="cb27-95"><a href="decision-trees.html#cb27-95" tabindex="-1"></a><span class="do">##   Surrogate splits:</span></span>
<span id="cb27-96"><a href="decision-trees.html#cb27-96" tabindex="-1"></a><span class="do">##       waistcirc    &lt; 98.75  to the left,  agree=0.84, adj=0.636, (0 split)</span></span>
<span id="cb27-97"><a href="decision-trees.html#cb27-97" tabindex="-1"></a><span class="do">##       elbowbreadth &lt; 6.45   to the left,  agree=0.76, adj=0.455, (0 split)</span></span>
<span id="cb27-98"><a href="decision-trees.html#cb27-98" tabindex="-1"></a><span class="do">##       kneebreadth  &lt; 8.75   to the left,  agree=0.68, adj=0.273, (0 split)</span></span>
<span id="cb27-99"><a href="decision-trees.html#cb27-99" tabindex="-1"></a><span class="do">##       age          &lt; 49.5   to the right, agree=0.60, adj=0.091, (0 split)</span></span>
<span id="cb27-100"><a href="decision-trees.html#cb27-100" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-101"><a href="decision-trees.html#cb27-101" tabindex="-1"></a><span class="do">## Node number 7: 3 observations</span></span>
<span id="cb27-102"><a href="decision-trees.html#cb27-102" tabindex="-1"></a><span class="do">##   mean=56.44667, MSE=48.76009 </span></span>
<span id="cb27-103"><a href="decision-trees.html#cb27-103" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-104"><a href="decision-trees.html#cb27-104" tabindex="-1"></a><span class="do">## Node number 8: 7 observations</span></span>
<span id="cb27-105"><a href="decision-trees.html#cb27-105" tabindex="-1"></a><span class="do">##   mean=16.12714, MSE=8.875049 </span></span>
<span id="cb27-106"><a href="decision-trees.html#cb27-106" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-107"><a href="decision-trees.html#cb27-107" tabindex="-1"></a><span class="do">## Node number 9: 4 observations</span></span>
<span id="cb27-108"><a href="decision-trees.html#cb27-108" tabindex="-1"></a><span class="do">##   mean=22.005, MSE=7.515725 </span></span>
<span id="cb27-109"><a href="decision-trees.html#cb27-109" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-110"><a href="decision-trees.html#cb27-110" tabindex="-1"></a><span class="do">## Node number 12: 11 observations</span></span>
<span id="cb27-111"><a href="decision-trees.html#cb27-111" tabindex="-1"></a><span class="do">##   mean=34.17545, MSE=7.173207 </span></span>
<span id="cb27-112"><a href="decision-trees.html#cb27-112" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-113"><a href="decision-trees.html#cb27-113" tabindex="-1"></a><span class="do">## Node number 13: 14 observations</span></span>
<span id="cb27-114"><a href="decision-trees.html#cb27-114" tabindex="-1"></a><span class="do">##   mean=42.48214, MSE=5.959931</span></span>
<span id="cb27-115"><a href="decision-trees.html#cb27-115" tabindex="-1"></a> <span class="fu">printcp</span>(body_model)</span>
<span id="cb27-116"><a href="decision-trees.html#cb27-116" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-117"><a href="decision-trees.html#cb27-117" tabindex="-1"></a><span class="do">## Regression tree:</span></span>
<span id="cb27-118"><a href="decision-trees.html#cb27-118" tabindex="-1"></a><span class="do">## rpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + </span></span>
<span id="cb27-119"><a href="decision-trees.html#cb27-119" tabindex="-1"></a><span class="do">##     kneebreadth, data = train, control = rpart.control(minsplit = 10))</span></span>
<span id="cb27-120"><a href="decision-trees.html#cb27-120" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-121"><a href="decision-trees.html#cb27-121" tabindex="-1"></a><span class="do">## Variables actually used in tree construction:</span></span>
<span id="cb27-122"><a href="decision-trees.html#cb27-122" tabindex="-1"></a><span class="do">## [1] age         hipcirc     kneebreadth waistcirc  </span></span>
<span id="cb27-123"><a href="decision-trees.html#cb27-123" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-124"><a href="decision-trees.html#cb27-124" tabindex="-1"></a><span class="do">## Root node error: 6707.2/53 = 126.55</span></span>
<span id="cb27-125"><a href="decision-trees.html#cb27-125" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-126"><a href="decision-trees.html#cb27-126" tabindex="-1"></a><span class="do">## n= 53 </span></span>
<span id="cb27-127"><a href="decision-trees.html#cb27-127" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-128"><a href="decision-trees.html#cb27-128" tabindex="-1"></a><span class="do">##         CP nsplit rel error  xerror     xstd</span></span>
<span id="cb27-129"><a href="decision-trees.html#cb27-129" tabindex="-1"></a><span class="do">## 1 0.674922      0  1.000000 1.03875 0.192982</span></span>
<span id="cb27-130"><a href="decision-trees.html#cb27-130" tabindex="-1"></a><span class="do">## 2 0.123979      1  0.325078 0.36747 0.097134</span></span>
<span id="cb27-131"><a href="decision-trees.html#cb27-131" tabindex="-1"></a><span class="do">## 3 0.063372      2  0.201099 0.32077 0.070554</span></span>
<span id="cb27-132"><a href="decision-trees.html#cb27-132" tabindex="-1"></a><span class="do">## 4 0.045395      3  0.137727 0.32984 0.070301</span></span>
<span id="cb27-133"><a href="decision-trees.html#cb27-133" tabindex="-1"></a><span class="do">## 5 0.013112      4  0.092332 0.26358 0.072159</span></span>
<span id="cb27-134"><a href="decision-trees.html#cb27-134" tabindex="-1"></a><span class="do">## 6 0.010000      5  0.079221 0.25025 0.069672</span></span>
<span id="cb27-135"><a href="decision-trees.html#cb27-135" tabindex="-1"></a> </span>
<span id="cb27-136"><a href="decision-trees.html#cb27-136" tabindex="-1"></a> body_model2<span class="ot">&lt;-</span><span class="fu">prune</span>(body_model,<span class="at">cp=</span><span class="fl">0.05175731</span>)</span>
<span id="cb27-137"><a href="decision-trees.html#cb27-137" tabindex="-1"></a> <span class="fu">printcp</span>(body_model2)</span>
<span id="cb27-138"><a href="decision-trees.html#cb27-138" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-139"><a href="decision-trees.html#cb27-139" tabindex="-1"></a><span class="do">## Regression tree:</span></span>
<span id="cb27-140"><a href="decision-trees.html#cb27-140" tabindex="-1"></a><span class="do">## rpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + </span></span>
<span id="cb27-141"><a href="decision-trees.html#cb27-141" tabindex="-1"></a><span class="do">##     kneebreadth, data = train, control = rpart.control(minsplit = 10))</span></span>
<span id="cb27-142"><a href="decision-trees.html#cb27-142" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-143"><a href="decision-trees.html#cb27-143" tabindex="-1"></a><span class="do">## Variables actually used in tree construction:</span></span>
<span id="cb27-144"><a href="decision-trees.html#cb27-144" tabindex="-1"></a><span class="do">## [1] hipcirc     kneebreadth waistcirc  </span></span>
<span id="cb27-145"><a href="decision-trees.html#cb27-145" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-146"><a href="decision-trees.html#cb27-146" tabindex="-1"></a><span class="do">## Root node error: 6707.2/53 = 126.55</span></span>
<span id="cb27-147"><a href="decision-trees.html#cb27-147" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-148"><a href="decision-trees.html#cb27-148" tabindex="-1"></a><span class="do">## n= 53 </span></span>
<span id="cb27-149"><a href="decision-trees.html#cb27-149" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb27-150"><a href="decision-trees.html#cb27-150" tabindex="-1"></a><span class="do">##         CP nsplit rel error  xerror     xstd</span></span>
<span id="cb27-151"><a href="decision-trees.html#cb27-151" tabindex="-1"></a><span class="do">## 1 0.674922      0   1.00000 1.03875 0.192982</span></span>
<span id="cb27-152"><a href="decision-trees.html#cb27-152" tabindex="-1"></a><span class="do">## 2 0.123979      1   0.32508 0.36747 0.097134</span></span>
<span id="cb27-153"><a href="decision-trees.html#cb27-153" tabindex="-1"></a><span class="do">## 3 0.063372      2   0.20110 0.32077 0.070554</span></span>
<span id="cb27-154"><a href="decision-trees.html#cb27-154" tabindex="-1"></a><span class="do">## 4 0.051757      3   0.13773 0.32984 0.070301</span></span>
<span id="cb27-155"><a href="decision-trees.html#cb27-155" tabindex="-1"></a> </span>
<span id="cb27-156"><a href="decision-trees.html#cb27-156" tabindex="-1"></a>varimp.data<span class="ot">=</span><span class="fu">data.frame</span>(body_model2<span class="sc">$</span>variable.importance)</span>
<span id="cb27-157"><a href="decision-trees.html#cb27-157" tabindex="-1"></a>varimp.data<span class="sc">$</span>names<span class="ot">=</span><span class="fu">as.character</span>(<span class="fu">rownames</span>(varimp.data))</span>
<span id="cb27-158"><a href="decision-trees.html#cb27-158" tabindex="-1"></a></span>
<span id="cb27-159"><a href="decision-trees.html#cb27-159" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>varimp.data,<span class="fu">aes</span>(<span class="at">x=</span>names,<span class="at">y=</span>body_model2.variable.importance))<span class="sc">+</span><span class="fu">geom_bar</span>(<span class="at">stat=</span><span class="st">&quot;identity&quot;</span>)<span class="sc">+</span><span class="fu">coord_flip</span>()<span class="sc">+</span><span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Variable Name&quot;</span>,<span class="at">y=</span><span class="st">&quot;Variable Importance&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Regression%20Trees-5.png" width="672" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="decision-trees.html#cb28-1" tabindex="-1"></a></span>
<span id="cb28-2"><a href="decision-trees.html#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="decision-trees.html#cb28-3" tabindex="-1"></a>tscores <span class="ot">=</span> <span class="fu">predict</span>(body_model2,<span class="at">type=</span><span class="st">&#39;vector&#39;</span>)</span>
<span id="cb28-4"><a href="decision-trees.html#cb28-4" tabindex="-1"></a>scores <span class="ot">=</span> <span class="fu">predict</span>(body_model2, test, <span class="at">type=</span><span class="st">&#39;vector&#39;</span>)</span>
<span id="cb28-5"><a href="decision-trees.html#cb28-5" tabindex="-1"></a></span>
<span id="cb28-6"><a href="decision-trees.html#cb28-6" tabindex="-1"></a><span class="do">##Training measures:</span></span>
<span id="cb28-7"><a href="decision-trees.html#cb28-7" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(tscores<span class="sc">-</span>train<span class="sc">$</span>DEXfat))</span>
<span id="cb28-8"><a href="decision-trees.html#cb28-8" tabindex="-1"></a><span class="do">## [1] 3.338013</span></span>
<span id="cb28-9"><a href="decision-trees.html#cb28-9" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>((tscores<span class="sc">-</span>train<span class="sc">$</span>DEXfat)<span class="sc">/</span>train<span class="sc">$</span>DEXfat))</span>
<span id="cb28-10"><a href="decision-trees.html#cb28-10" tabindex="-1"></a><span class="do">## [1] 0.1357316</span></span>
<span id="cb28-11"><a href="decision-trees.html#cb28-11" tabindex="-1"></a></span>
<span id="cb28-12"><a href="decision-trees.html#cb28-12" tabindex="-1"></a><span class="do">### Test data:</span></span>
<span id="cb28-13"><a href="decision-trees.html#cb28-13" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(scores<span class="sc">-</span>test<span class="sc">$</span>DEXfat))</span>
<span id="cb28-14"><a href="decision-trees.html#cb28-14" tabindex="-1"></a><span class="do">## [1] 5.622164</span></span>
<span id="cb28-15"><a href="decision-trees.html#cb28-15" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>((scores<span class="sc">-</span>test<span class="sc">$</span>DEXfat)<span class="sc">/</span>test<span class="sc">$</span>DEXfat))</span>
<span id="cb28-16"><a href="decision-trees.html#cb28-16" tabindex="-1"></a><span class="do">## [1] 0.2492064</span></span>
<span id="cb28-17"><a href="decision-trees.html#cb28-17" tabindex="-1"></a></span>
<span id="cb28-18"><a href="decision-trees.html#cb28-18" tabindex="-1"></a></span>
<span id="cb28-19"><a href="decision-trees.html#cb28-19" tabindex="-1"></a><span class="fu">rpart.plot</span>(body_model2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Regression%20Trees-6.png" width="672" /></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="decision-trees.html#cb29-1" tabindex="-1"></a><span class="fu">rsq.rpart</span>(body_model2)</span>
<span id="cb29-2"><a href="decision-trees.html#cb29-2" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb29-3"><a href="decision-trees.html#cb29-3" tabindex="-1"></a><span class="do">## Regression tree:</span></span>
<span id="cb29-4"><a href="decision-trees.html#cb29-4" tabindex="-1"></a><span class="do">## rpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + </span></span>
<span id="cb29-5"><a href="decision-trees.html#cb29-5" tabindex="-1"></a><span class="do">##     kneebreadth, data = train, control = rpart.control(minsplit = 10))</span></span>
<span id="cb29-6"><a href="decision-trees.html#cb29-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb29-7"><a href="decision-trees.html#cb29-7" tabindex="-1"></a><span class="do">## Variables actually used in tree construction:</span></span>
<span id="cb29-8"><a href="decision-trees.html#cb29-8" tabindex="-1"></a><span class="do">## [1] hipcirc     kneebreadth waistcirc  </span></span>
<span id="cb29-9"><a href="decision-trees.html#cb29-9" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb29-10"><a href="decision-trees.html#cb29-10" tabindex="-1"></a><span class="do">## Root node error: 6707.2/53 = 126.55</span></span>
<span id="cb29-11"><a href="decision-trees.html#cb29-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb29-12"><a href="decision-trees.html#cb29-12" tabindex="-1"></a><span class="do">## n= 53 </span></span>
<span id="cb29-13"><a href="decision-trees.html#cb29-13" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb29-14"><a href="decision-trees.html#cb29-14" tabindex="-1"></a><span class="do">##         CP nsplit rel error  xerror     xstd</span></span>
<span id="cb29-15"><a href="decision-trees.html#cb29-15" tabindex="-1"></a><span class="do">## 1 0.674922      0   1.00000 1.03875 0.192982</span></span>
<span id="cb29-16"><a href="decision-trees.html#cb29-16" tabindex="-1"></a><span class="do">## 2 0.123979      1   0.32508 0.36747 0.097134</span></span>
<span id="cb29-17"><a href="decision-trees.html#cb29-17" tabindex="-1"></a><span class="do">## 3 0.063372      2   0.20110 0.32077 0.070554</span></span>
<span id="cb29-18"><a href="decision-trees.html#cb29-18" tabindex="-1"></a><span class="do">## 4 0.051757      3   0.13773 0.32984 0.070301</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Regression%20Trees-7.png" width="672" /><img src="bookdownproj_files/figure-html/Regression%20Trees-8.png" width="672" /></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="decision-trees.html#cb30-1" tabindex="-1"></a></span>
<span id="cb30-2"><a href="decision-trees.html#cb30-2" tabindex="-1"></a><span class="do">###Another nice plot</span></span>
<span id="cb30-3"><a href="decision-trees.html#cb30-3" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(body_model2, <span class="at">uniform=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Regression%20Trees-9.png" width="672" /></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="decision-trees.html#cb31-1" tabindex="-1"></a></span>
<span id="cb31-2"><a href="decision-trees.html#cb31-2" tabindex="-1"></a></span>
<span id="cb31-3"><a href="decision-trees.html#cb31-3" tabindex="-1"></a><span class="do">##Can also visualize the cp values</span></span>
<span id="cb31-4"><a href="decision-trees.html#cb31-4" tabindex="-1"></a></span>
<span id="cb31-5"><a href="decision-trees.html#cb31-5" tabindex="-1"></a><span class="fu">plotcp</span>(body_model)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Regression%20Trees-10.png" width="672" /></p>
<div id="regression-trees-in-python" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Regression trees in Python<a href="decision-trees.html#regression-trees-in-python" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can also do regression trees in Python:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="decision-trees.html#cb32-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb32-2"><a href="decision-trees.html#cb32-2" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb32-3"><a href="decision-trees.html#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a href="decision-trees.html#cb32-4" tabindex="-1"></a>train_py <span class="op">=</span> r.train</span>
<span id="cb32-5"><a href="decision-trees.html#cb32-5" tabindex="-1"></a>test_py <span class="op">=</span> r.test</span>
<span id="cb32-6"><a href="decision-trees.html#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a href="decision-trees.html#cb32-7" tabindex="-1"></a>X_train <span class="op">=</span> train_py[[<span class="st">&#39;age&#39;</span>, <span class="st">&#39;waistcirc&#39;</span>, <span class="st">&#39;hipcirc&#39;</span>, <span class="st">&#39;elbowbreadth&#39;</span>, <span class="st">&#39;kneebreadth&#39;</span>]]</span>
<span id="cb32-8"><a href="decision-trees.html#cb32-8" tabindex="-1"></a>X_test <span class="op">=</span> test_py[[<span class="st">&#39;age&#39;</span>, <span class="st">&#39;waistcirc&#39;</span>, <span class="st">&#39;hipcirc&#39;</span>,</span>
<span id="cb32-9"><a href="decision-trees.html#cb32-9" tabindex="-1"></a>  <span class="st">&#39;elbowbreadth&#39;</span>, <span class="st">&#39;kneebreadth&#39;</span>]]</span>
<span id="cb32-10"><a href="decision-trees.html#cb32-10" tabindex="-1"></a>y_train <span class="op">=</span> train_py[<span class="st">&#39;DEXfat&#39;</span>]</span>
<span id="cb32-11"><a href="decision-trees.html#cb32-11" tabindex="-1"></a>y_test <span class="op">=</span> test_py[<span class="st">&#39;DEXfat&#39;</span>]</span>
<span id="cb32-12"><a href="decision-trees.html#cb32-12" tabindex="-1"></a></span>
<span id="cb32-13"><a href="decision-trees.html#cb32-13" tabindex="-1"></a>regressor <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">12356</span>,max_depth<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb32-14"><a href="decision-trees.html#cb32-14" tabindex="-1"></a>reg_tree<span class="op">=</span>regressor.fit(X_train,y_train)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="decision-trees.html#cb34-1" tabindex="-1"></a>importance <span class="op">=</span> regressor.feature_importances_</span>
<span id="cb34-2"><a href="decision-trees.html#cb34-2" tabindex="-1"></a></span>
<span id="cb34-3"><a href="decision-trees.html#cb34-3" tabindex="-1"></a><span class="cf">for</span> i,v <span class="kw">in</span> <span class="bu">enumerate</span>(importance):</span>
<span id="cb34-4"><a href="decision-trees.html#cb34-4" tabindex="-1"></a> <span class="bu">print</span>(<span class="st">&#39;Feature: </span><span class="sc">%0d</span><span class="st">, Score: </span><span class="sc">%.5f</span><span class="st">&#39;</span> <span class="op">%</span> (i,v))</span></code></pre></div>
<pre><code>## Feature: 0, Score: 0.01353
## Feature: 1, Score: 0.70797
## Feature: 2, Score: 0.12906
## Feature: 3, Score: 0.00809
## Feature: 4, Score: 0.14136</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="decision-trees.html#cb36-1" tabindex="-1"></a>error_vec<span class="op">=</span>y_test<span class="op">-</span>reg_tree.predict(X_test)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="decision-trees.html#cb38-1" tabindex="-1"></a>mae<span class="op">=</span>np.mean(np.<span class="bu">abs</span>(error_vec))</span>
<span id="cb38-2"><a href="decision-trees.html#cb38-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;MAE is &#39;</span>,mae )</span></code></pre></div>
<pre><code>## MAE is  3.7088580246913585</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="decision-trees.html#cb40-1" tabindex="-1"></a>tree.plot_tree(reg_tree)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
</div>
<div id="recursive-partitioning-with-partykit" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Recursive partitioning with partykit<a href="decision-trees.html#recursive-partitioning-with-partykit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="decision-trees.html#cb41-1" tabindex="-1"></a><span class="do">### Classification example:</span></span>
<span id="cb41-2"><a href="decision-trees.html#cb41-2" tabindex="-1"></a></span>
<span id="cb41-3"><a href="decision-trees.html#cb41-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7515</span>)</span>
<span id="cb41-4"><a href="decision-trees.html#cb41-4" tabindex="-1"></a>perm<span class="ot">=</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">699</span>)</span>
<span id="cb41-5"><a href="decision-trees.html#cb41-5" tabindex="-1"></a>BC_randomOrder<span class="ot">=</span>BCdata[perm,]</span>
<span id="cb41-6"><a href="decision-trees.html#cb41-6" tabindex="-1"></a>train <span class="ot">=</span> BC_randomOrder[<span class="dv">1</span><span class="sc">:</span><span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="dv">699</span>),]</span>
<span id="cb41-7"><a href="decision-trees.html#cb41-7" tabindex="-1"></a>model1<span class="ot">=</span><span class="fu">ctree</span>(Target <span class="sc">~</span> . <span class="sc">-</span> ID, <span class="at">data=</span>train)</span>
<span id="cb41-8"><a href="decision-trees.html#cb41-8" tabindex="-1"></a>model1</span></code></pre></div>
<pre><code>## 
## Model formula:
## Target ~ CT + Size + Shape + Margin + Epithelial + Bare + Chromatin + 
##     Normal + Mitoses
## 
## Fitted party:
## [1] root
## |   [2] Shape &lt;= 3
## |   |   [3] Bare &lt;= 5
## |   |   |   [4] Size &lt;= 2
## |   |   |   |   [5] Bare &lt;= 2: 0.000 (n = 289, err = 0.0)
## |   |   |   |   [6] Bare &gt; 2
## |   |   |   |   |   [7] CT &lt;= 2: 0.000 (n = 9, err = 0.0)
## |   |   |   |   |   [8] CT &gt; 2: 0.200 (n = 10, err = 1.6)
## |   |   |   [9] Size &gt; 2
## |   |   |   |   [10] Normal &lt;= 3: 0.062 (n = 16, err = 0.9)
## |   |   |   |   [11] Normal &gt; 3: 0.714 (n = 7, err = 1.4)
## |   |   [12] Bare &gt; 5: 0.889 (n = 18, err = 1.8)
## |   [13] Shape &gt; 3
## |   |   [14] Size &lt;= 4
## |   |   |   [15] CT &lt;= 5
## |   |   |   |   [16] Margin &lt;= 3: 0.083 (n = 12, err = 0.9)
## |   |   |   |   [17] Margin &gt; 3: 0.778 (n = 9, err = 1.6)
## |   |   |   [18] CT &gt; 5: 0.963 (n = 27, err = 1.0)
## |   |   [19] Size &gt; 4: 0.984 (n = 127, err = 2.0)
## 
## Number of inner nodes:     9
## Number of terminal nodes: 10</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="decision-trees.html#cb43-1" tabindex="-1"></a><span class="fu">plot</span>(model1)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="decision-trees.html#cb44-1" tabindex="-1"></a><span class="do">###Regression example:</span></span>
<span id="cb44-2"><a href="decision-trees.html#cb44-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">13172</span>) </span>
<span id="cb44-3"><a href="decision-trees.html#cb44-3" tabindex="-1"></a>sample <span class="ot">&lt;-</span> <span class="fu">sample.int</span>(<span class="at">n =</span> <span class="fu">nrow</span>(bodyfat), <span class="at">size =</span> <span class="fu">floor</span>(.<span class="dv">75</span><span class="sc">*</span><span class="fu">nrow</span>(bodyfat)), <span class="at">replace =</span> F)</span>
<span id="cb44-4"><a href="decision-trees.html#cb44-4" tabindex="-1"></a>train <span class="ot">&lt;-</span> bodyfat[sample, ]</span>
<span id="cb44-5"><a href="decision-trees.html#cb44-5" tabindex="-1"></a>model1<span class="ot">&lt;-</span><span class="fu">ctree</span>(DEXfat <span class="sc">~</span> age <span class="sc">+</span> waistcirc <span class="sc">+</span> hipcirc <span class="sc">+</span> elbowbreadth <span class="sc">+</span> </span>
<span id="cb44-6"><a href="decision-trees.html#cb44-6" tabindex="-1"></a>                kneebreadth, <span class="at">data =</span> train)</span>
<span id="cb44-7"><a href="decision-trees.html#cb44-7" tabindex="-1"></a>model1</span></code></pre></div>
<pre><code>## 
## Model formula:
## DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth
## 
## Fitted party:
## [1] root
## |   [2] waistcirc &lt;= 86
## |   |   [3] hipcirc &lt;= 96: 18.265 (n = 11, err = 180.1)
## |   |   [4] hipcirc &gt; 96: 25.295 (n = 14, err = 130.5)
## |   [5] waistcirc &gt; 86
## |   |   [6] hipcirc &lt;= 109.5: 34.175 (n = 11, err = 78.9)
## |   |   [7] hipcirc &gt; 109.5: 44.946 (n = 17, err = 711.5)
## 
## Number of inner nodes:    3
## Number of terminal nodes: 4</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="decision-trees.html#cb46-1" tabindex="-1"></a><span class="fu">plot</span>(model1)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-4-4.png" width="672" /></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="decision-trees.html#cb47-1" tabindex="-1"></a><span class="do">### Example for binning data:</span></span>
<span id="cb47-2"><a href="decision-trees.html#cb47-2" tabindex="-1"></a>churn<span class="ot">=</span><span class="fu">read.csv</span>(<span class="st">&quot;Q:</span><span class="sc">\\</span><span class="st">My Drive</span><span class="sc">\\</span><span class="st">Data Mining</span><span class="sc">\\</span><span class="st">Data</span><span class="sc">\\</span><span class="st">tele_churn.csv&quot;</span>)</span>
<span id="cb47-3"><a href="decision-trees.html#cb47-3" tabindex="-1"></a>churn<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">ifelse</span>(churn<span class="sc">$</span>churn<span class="sc">==</span><span class="st">&quot;TRUE&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb47-4"><a href="decision-trees.html#cb47-4" tabindex="-1"></a>churn<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">ordered</span>(churn<span class="sc">$</span>y,<span class="at">levels=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">labels=</span><span class="st">&quot;No&quot;</span>,<span class="st">&quot;Yes&quot;</span>)</span>
<span id="cb47-5"><a href="decision-trees.html#cb47-5" tabindex="-1"></a>model1<span class="ot">&lt;-</span><span class="fu">ctree</span>(y<span class="sc">~</span>total.day.minutes,<span class="at">data=</span>churn)</span>
<span id="cb47-6"><a href="decision-trees.html#cb47-6" tabindex="-1"></a>model1</span></code></pre></div>
<pre><code>## 
## Model formula:
## y ~ total.day.minutes
## 
## Fitted party:
## [1] root
## |   [2] total.day.minutes &lt;= 283.9
## |   |   [3] total.day.minutes &lt;= 245: No1 (n = 2739, err = 3.4%)
## |   |   [4] total.day.minutes &gt; 245: No1 (n = 216, err = 15.7%)
## |   [5] total.day.minutes &gt; 283.9: No2 (n = 49, err = 46.9%)
## 
## Number of inner nodes:    2
## Number of terminal nodes: 3</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="decision-trees.html#cb49-1" tabindex="-1"></a><span class="fu">plot</span>(model1)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-4-5.png" width="672" /></p>
<p>Some interesting extra tidbits!! This shows how to do ROC curves and lift curves for classification trees.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="decision-trees.html#cb50-1" tabindex="-1"></a>test <span class="ot">=</span> BC_randomOrder[(<span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="dv">699</span>)<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">699</span>,]</span>
<span id="cb50-2"><a href="decision-trees.html#cb50-2" tabindex="-1"></a><span class="do">###Lift (from classification trees)</span></span>
<span id="cb50-3"><a href="decision-trees.html#cb50-3" tabindex="-1"></a>scores1<span class="ot">=</span><span class="fu">predict</span>(BC.tree,test,<span class="at">type=</span><span class="st">&quot;prob&quot;</span>)</span>
<span id="cb50-4"><a href="decision-trees.html#cb50-4" tabindex="-1"></a>pred_val <span class="ot">&lt;-</span><span class="fu">prediction</span>(scores1[,<span class="dv">2</span>],test<span class="sc">$</span>Target)</span>
<span id="cb50-5"><a href="decision-trees.html#cb50-5" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">performance</span>(pred_val, <span class="at">measure=</span><span class="st">&quot;lift&quot;</span>, <span class="at">x.measure=</span><span class="st">&quot;rpp&quot;</span>), <span class="at">colorize=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/extras-1.png" width="672" /></p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="decision-trees.html#cb51-1" tabindex="-1"></a><span class="co"># Calculating True Positive and False Positive Rate</span></span>
<span id="cb51-2"><a href="decision-trees.html#cb51-2" tabindex="-1"></a>perf_val <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_val, <span class="st">&quot;tpr&quot;</span>, <span class="st">&quot;fpr&quot;</span>)</span>
<span id="cb51-3"><a href="decision-trees.html#cb51-3" tabindex="-1"></a><span class="co">#Plot the ROC curve</span></span>
<span id="cb51-4"><a href="decision-trees.html#cb51-4" tabindex="-1"></a><span class="fu">plot</span>(perf_val, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/extras-2.png" width="672" /></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="decision-trees.html#cb52-1" tabindex="-1"></a><span class="co">#Calculating KS statistics</span></span>
<span id="cb52-2"><a href="decision-trees.html#cb52-2" tabindex="-1"></a>ks1.tree <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">attr</span>(perf_val, <span class="st">&quot;y.values&quot;</span>)[[<span class="dv">1</span>]] <span class="sc">-</span> (<span class="fu">attr</span>(perf_val, <span class="st">&quot;x.values&quot;</span>)[[<span class="dv">1</span>]]))</span>
<span id="cb52-3"><a href="decision-trees.html#cb52-3" tabindex="-1"></a>ks1.tree</span></code></pre></div>
<pre><code>## [1] 0.8971412</code></pre>
<div id="python-for-conditional-inference-decision-trees" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Python for Conditional Inference Decision Trees<a href="decision-trees.html#python-for-conditional-inference-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sorry, there is not a good algorithm to currently do this in Python.</p>
</div>
</div>
<div id="model-reliance" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Model Reliance<a href="decision-trees.html#model-reliance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Model reliance is a model agnostic value for variable importance. It is the ratio of the expected loss of a model when ânoiseâ is incorporated into a model versus the expect loss of the model. For incorporating ânoiseâ into the model, we will use Dr.Â Breimanâs permutation of a variable in the model. To explore the importance of variable 1, we will permutet the values of variable 1 and compare its expected loss to the expect loss when this variable is not permuted. We will do this for all variables in the data set. We will go back to the original decision tree:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="decision-trees.html#cb54-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7515</span>)</span>
<span id="cb54-2"><a href="decision-trees.html#cb54-2" tabindex="-1"></a>perm<span class="ot">=</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">699</span>)</span>
<span id="cb54-3"><a href="decision-trees.html#cb54-3" tabindex="-1"></a>BC_randomOrder<span class="ot">=</span>BCdata[perm,]</span>
<span id="cb54-4"><a href="decision-trees.html#cb54-4" tabindex="-1"></a>train <span class="ot">=</span> BC_randomOrder[<span class="dv">1</span><span class="sc">:</span><span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="dv">699</span>),]</span>
<span id="cb54-5"><a href="decision-trees.html#cb54-5" tabindex="-1"></a>test <span class="ot">=</span> BC_randomOrder[(<span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="dv">699</span>)<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">699</span>,]</span>
<span id="cb54-6"><a href="decision-trees.html#cb54-6" tabindex="-1"></a>BC.tree <span class="ot">=</span> <span class="fu">rpart</span>(Target <span class="sc">~</span> . <span class="sc">-</span> ID, <span class="at">data=</span>train, <span class="at">method=</span><span class="st">&#39;class&#39;</span>,</span>
<span id="cb54-7"><a href="decision-trees.html#cb54-7" tabindex="-1"></a> <span class="at">parms =</span> <span class="fu">list</span>(<span class="at">split=</span><span class="st">&#39;gini&#39;</span>)) <span class="do">## or &#39;information&#39;</span></span>
<span id="cb54-8"><a href="decision-trees.html#cb54-8" tabindex="-1"></a></span>
<span id="cb54-9"><a href="decision-trees.html#cb54-9" tabindex="-1"></a>VI <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">length=</span><span class="fu">ncol</span>(train)<span class="sc">-</span><span class="dv">2</span>)</span>
<span id="cb54-10"><a href="decision-trees.html#cb54-10" tabindex="-1"></a>loss.model<span class="ot">=</span><span class="fu">mean</span>(<span class="fu">abs</span>(train<span class="sc">$</span>Target<span class="sc">-</span><span class="fu">as.numeric</span>(<span class="fu">as.character</span>(<span class="fu">predict</span>(BC.tree,<span class="at">type=</span><span class="st">&quot;class&quot;</span>)))))</span>
<span id="cb54-11"><a href="decision-trees.html#cb54-11" tabindex="-1"></a>temp1<span class="ot">=</span>train</span>
<span id="cb54-12"><a href="decision-trees.html#cb54-12" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb54-13"><a href="decision-trees.html#cb54-13" tabindex="-1"></a>  {temp1<span class="ot">=</span>train</span>
<span id="cb54-14"><a href="decision-trees.html#cb54-14" tabindex="-1"></a>   temp1[,j]<span class="ot">=</span><span class="fu">sample</span>(train[,j])</span>
<span id="cb54-15"><a href="decision-trees.html#cb54-15" tabindex="-1"></a>   loss.noise <span class="ot">=</span> <span class="fu">mean</span>(<span class="fu">abs</span>(train<span class="sc">$</span>Target<span class="sc">-</span><span class="fu">as.numeric</span>(<span class="fu">as.character</span>(<span class="fu">predict</span>(BC.tree, <span class="at">newdata=</span>temp1,<span class="at">type=</span><span class="st">&quot;class&quot;</span>)))))</span>
<span id="cb54-16"><a href="decision-trees.html#cb54-16" tabindex="-1"></a>   VI[(j<span class="dv">-1</span>)] <span class="ot">=</span> loss.noise<span class="sc">/</span>loss.model</span>
<span id="cb54-17"><a href="decision-trees.html#cb54-17" tabindex="-1"></a>}</span>
<span id="cb54-18"><a href="decision-trees.html#cb54-18" tabindex="-1"></a></span>
<span id="cb54-19"><a href="decision-trees.html#cb54-19" tabindex="-1"></a>VI<span class="ot">&lt;-</span><span class="fu">data.frame</span>(VI)</span>
<span id="cb54-20"><a href="decision-trees.html#cb54-20" tabindex="-1"></a><span class="fu">rownames</span>(VI)<span class="ot">&lt;-</span><span class="fu">colnames</span>(train[<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>])</span>
<span id="cb54-21"><a href="decision-trees.html#cb54-21" tabindex="-1"></a>VI</span></code></pre></div>
<pre><code>##                  VI
## CT         1.764706
## Size       6.588235
## Shape      1.000000
## Margin     1.000000
## Epithelial 1.000000
## Bare       3.352941
## Chromatin  1.000000
## Normal     6.117647
## Mitoses    1.000000</code></pre>
<p>Please NOTE that Size is the most important, followed by Normal, then Bare, and finally CT. This is the exact order of the decision tree (which makes sense why these are the most important and the order of importance). Since model reliance is a model agnostic procedure, you can use this on other models as well. Good introduction to some of the other measures you will see in Machine Learning.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sjsimmo2/DataMining-Fall/edit/master/02-Decision-Trees.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/sjsimmo2/DataMining-Fall/blob/master/02-Decision-Trees.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
