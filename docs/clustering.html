<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Clustering | Data Mining</title>
  <meta name="description" content="Chapter 4 Clustering | Data Mining" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Clustering | Data Mining" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Clustering | Data Mining" />
  
  
  

<meta name="author" content="Dr. Susan Simmons" />


<meta name="date" content="2023-10-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="decision-trees.html"/>
<link rel="next" href="other-topics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">DataMining</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Data Mining</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#bootstrapping"><i class="fa fa-check"></i><b>2.1</b> Bootstrapping</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction.html"><a href="introduction.html#python-for-bootstrapping"><i class="fa fa-check"></i><b>2.1.1</b> Python for Bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#adjusting-p-values"><i class="fa fa-check"></i><b>2.2</b> Adjusting p-values</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction.html"><a href="introduction.html#python-code-for-fdr"><i class="fa fa-check"></i><b>2.2.1</b> Python code for FDR</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#transaction-data"><i class="fa fa-check"></i><b>2.3</b> Transaction data</a></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#association-analysis"><i class="fa fa-check"></i><b>2.4</b> Association Analysis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction.html"><a href="introduction.html#python-code-for-association-analysis"><i class="fa fa-check"></i><b>2.4.1</b> Python code for Association Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>3</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.1" data-path="decision-trees.html"><a href="decision-trees.html#classification-trees"><i class="fa fa-check"></i><b>3.1</b> Classification Trees</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="decision-trees.html"><a href="decision-trees.html#python-classification-trees"><i class="fa fa-check"></i><b>3.1.1</b> Python Classification Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees"><i class="fa fa-check"></i><b>3.2</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="decision-trees.html"><a href="decision-trees.html#regression-trees-in-python"><i class="fa fa-check"></i><b>3.2.1</b> Regression trees in Python</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="decision-trees.html"><a href="decision-trees.html#recursive-partitioning-with-partykit"><i class="fa fa-check"></i><b>3.3</b> Recursive partitioning with partykit</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="decision-trees.html"><a href="decision-trees.html#python-for-conditional-inference-decision-trees"><i class="fa fa-check"></i><b>3.3.1</b> Python for Conditional Inference Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="decision-trees.html"><a href="decision-trees.html#model-reliance"><i class="fa fa-check"></i><b>3.4</b> Model Reliance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>4</b> Clustering</a>
<ul>
<li class="chapter" data-level="4.1" data-path="clustering.html"><a href="clustering.html#kmeans"><i class="fa fa-check"></i><b>4.1</b> Kmeans</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="clustering.html"><a href="clustering.html#k-means-in-python"><i class="fa fa-check"></i><b>4.1.1</b> K-Means in Python</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>4.2</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="clustering.html"><a href="clustering.html#python-for-hierarchical-clustering"><i class="fa fa-check"></i><b>4.2.1</b> Python for Hierarchical Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="clustering.html"><a href="clustering.html#variable-clustering"><i class="fa fa-check"></i><b>4.3</b> Variable Clustering</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="other-topics.html"><a href="other-topics.html"><i class="fa fa-check"></i><b>5</b> Other Topics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="other-topics.html"><a href="other-topics.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.1</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2" data-path="other-topics.html"><a href="other-topics.html#mds"><i class="fa fa-check"></i><b>5.2</b> MDS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Mining</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Clustering<a href="clustering.html#clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We will now get into clustering. There are two broad areas of clustering: hard and soft. Hard clustering is when an observation can belong to one and only one cluster. Soft clustering is when an observation receives some type of measure (for example, a probability) of being in a cluster. We will be focusing on hard clustering.</p>
<p>Within hard clustering, algorithms can be flat (for example, kmeans) or hierarchical. Finally, within hierarchical, we have agglomeratie and divisive (just like they sound…agglomerative means we will build things up where each observations starts as its own cluster; divisive means we will tear things down which means all observations start in one cluster).</p>
<p>We will start by looking at kmeans, then move into hierarchical clustering.</p>
<div id="kmeans" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Kmeans<a href="clustering.html#kmeans" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will be using the USArrests data set (this dat is already in R). It is an older data set, but still good to illustrate clustering. This data has 50 observations (one observation per state) and contains statistics in arrests per 100,000 residents for assault, murder and rape. Also provided is the percent of the population in that state who live in urban areas.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="clustering.html#cb1-1" tabindex="-1"></a><span class="fu">summary</span>(USArrests)</span></code></pre></div>
<pre><code>##      Murder          Assault         UrbanPop          Rape      
##  Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30  
##  1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50   1st Qu.:15.07  
##  Median : 7.250   Median :159.0   Median :66.00   Median :20.10  
##  Mean   : 7.788   Mean   :170.8   Mean   :65.54   Mean   :21.23  
##  3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75   3rd Qu.:26.18  
##  Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="clustering.html#cb3-1" tabindex="-1"></a><span class="fu">hist</span>(USArrests<span class="sc">$</span>Murder)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cluster%20preprocess-1.png" width="672" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="clustering.html#cb4-1" tabindex="-1"></a><span class="fu">hist</span>(USArrests<span class="sc">$</span>Assault)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cluster%20preprocess-2.png" width="672" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="clustering.html#cb5-1" tabindex="-1"></a><span class="fu">hist</span>(USArrests<span class="sc">$</span>Rape)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cluster%20preprocess-3.png" width="672" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="clustering.html#cb6-1" tabindex="-1"></a><span class="fu">hist</span>(USArrests<span class="sc">$</span>UrbanPop)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cluster%20preprocess-4.png" width="672" /></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="clustering.html#cb7-1" tabindex="-1"></a>arrest.scal<span class="ot">=</span><span class="fu">scale</span>(USArrests)</span></code></pre></div>
<p>With only 4 variables and good scales, we will just use the scaled data (arrest.scal) for the clustering.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="clustering.html#cb8-1" tabindex="-1"></a>clus2<span class="ot">=</span><span class="fu">kmeans</span>(arrest.scal,<span class="at">centers=</span><span class="dv">2</span>,<span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb8-2"><a href="clustering.html#cb8-2" tabindex="-1"></a>clus2</span></code></pre></div>
<pre><code>## K-means clustering with 2 clusters of sizes 30, 20
## 
## Cluster means:
##      Murder    Assault   UrbanPop       Rape
## 1 -0.669956 -0.6758849 -0.1317235 -0.5646433
## 2  1.004934  1.0138274  0.1975853  0.8469650
## 
## Clustering vector:
##        Alabama         Alaska        Arizona       Arkansas     California 
##              2              2              2              1              2 
##       Colorado    Connecticut       Delaware        Florida        Georgia 
##              2              1              1              2              2 
##         Hawaii          Idaho       Illinois        Indiana           Iowa 
##              1              1              2              1              1 
##         Kansas       Kentucky      Louisiana          Maine       Maryland 
##              1              1              2              1              2 
##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
##              1              2              1              2              2 
##        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
##              1              1              2              1              1 
##     New Mexico       New York North Carolina   North Dakota           Ohio 
##              2              2              2              1              1 
##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
##              1              1              1              1              2 
##   South Dakota      Tennessee          Texas           Utah        Vermont 
##              1              2              2              1              1 
##       Virginia     Washington  West Virginia      Wisconsin        Wyoming 
##              1              1              1              1              1 
## 
## Within cluster sum of squares by cluster:
## [1] 56.11445 46.74796
##  (between_SS / total_SS =  47.5 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot;
## [6] &quot;betweenss&quot;    &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="clustering.html#cb10-1" tabindex="-1"></a><span class="fu">fviz_cluster</span>(clus2, <span class="at">data =</span> arrest.scal)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/kmeans-1.png" width="672" /></p>
<p>We do not know if 2 clusters is the best number of clusters, so we will need to explore other options and determine which is the best number of clusters for this data set.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="clustering.html#cb11-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12964</span>)</span>
<span id="cb11-2"><a href="clustering.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="clustering.html#cb11-3" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(arrest.scal, kmeans, <span class="at">method =</span> <span class="st">&quot;wss&quot;</span>,<span class="at">k.max =</span> <span class="dv">9</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/numofclusters-1.png" width="672" /></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="clustering.html#cb12-1" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(arrest.scal, kmeans, <span class="at">method =</span> <span class="st">&quot;silhouette&quot;</span>,<span class="at">k.max =</span> <span class="dv">9</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/numofclusters-2.png" width="672" /></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="clustering.html#cb13-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb13-2"><a href="clustering.html#cb13-2" tabindex="-1"></a>gap_stat <span class="ot">=</span> <span class="fu">clusGap</span>(arrest.scal, <span class="at">FUN =</span> kmeans, <span class="at">nstart =</span> <span class="dv">25</span>, <span class="at">K.max =</span> <span class="dv">9</span>, <span class="at">B =</span> <span class="dv">50</span>)</span>
<span id="cb13-3"><a href="clustering.html#cb13-3" tabindex="-1"></a><span class="fu">fviz_gap_stat</span>(gap_stat)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/numofclusters-3.png" width="672" /></p>
<p>As you can see, different statistics provide different number of clusters (of course!). As a final thought, let’s plot all of these using PCA.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="clustering.html#cb14-1" tabindex="-1"></a>k2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(arrest.scal, <span class="at">centers =</span> <span class="dv">2</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb14-2"><a href="clustering.html#cb14-2" tabindex="-1"></a>k3 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(arrest.scal, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb14-3"><a href="clustering.html#cb14-3" tabindex="-1"></a>k4 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(arrest.scal, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb14-4"><a href="clustering.html#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a href="clustering.html#cb14-5" tabindex="-1"></a><span class="co"># plots to compare</span></span>
<span id="cb14-6"><a href="clustering.html#cb14-6" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">fviz_cluster</span>(k2, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">data =</span> arrest.scal) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;k = 2&quot;</span>)</span>
<span id="cb14-7"><a href="clustering.html#cb14-7" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">fviz_cluster</span>(k3, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>,  <span class="at">data =</span> arrest.scal) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;k = 3&quot;</span>)</span>
<span id="cb14-8"><a href="clustering.html#cb14-8" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">fviz_cluster</span>(k4, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>,  <span class="at">data =</span> arrest.scal) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;k = 4&quot;</span>)</span>
<span id="cb14-9"><a href="clustering.html#cb14-9" tabindex="-1"></a></span>
<span id="cb14-10"><a href="clustering.html#cb14-10" tabindex="-1"></a></span>
<span id="cb14-11"><a href="clustering.html#cb14-11" tabindex="-1"></a><span class="fu">grid.arrange</span>(p2, p3, p4, <span class="at">nrow =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/visualize%20clusters-1.png" width="672" /></p>
<p>Going with 2 clusters, we can now try to profile the data. First, we need to merge the data with the cluster numbers.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="clustering.html#cb15-1" tabindex="-1"></a>profile.kmeans<span class="ot">=</span><span class="fu">cbind</span>(USArrests,k2<span class="sc">$</span>cluster)</span>
<span id="cb15-2"><a href="clustering.html#cb15-2" tabindex="-1"></a>all.k<span class="ot">=</span>profile.kmeans <span class="sc">%&gt;%</span> <span class="fu">group_by</span>(k2<span class="sc">$</span>cluster) <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="clustering.html#cb15-3" tabindex="-1"></a>   <span class="fu">summarise</span>(<span class="at">mean.assault=</span><span class="fu">mean</span>(Assault),<span class="at">mean.murder=</span><span class="fu">mean</span>(Murder),<span class="at">mean.rape=</span><span class="fu">mean</span>(Rape),<span class="at">mean.pop=</span><span class="fu">mean</span>(UrbanPop))</span>
<span id="cb15-4"><a href="clustering.html#cb15-4" tabindex="-1"></a> all.k</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   `k2$cluster` mean.assault mean.murder mean.rape mean.pop
##          &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1            1         114.        4.87      15.9     63.6
## 2            2         255.       12.2       29.2     68.4</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="clustering.html#cb17-1" tabindex="-1"></a> <span class="fu">NbClust</span>(arrest.scal,<span class="at">method=</span><span class="st">&quot;kmeans&quot;</span>,<span class="at">min.nc=</span><span class="dv">2</span>,<span class="at">max.nc =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/kmeans%20profile-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="bookdownproj_files/figure-html/kmeans%20profile-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 13 proposed 2 as the best number of clusters 
## * 9 proposed 3 as the best number of clusters 
## * 2 proposed 4 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
<pre><code>## $All.index
##       KL      CH Hartigan     CCC    Scott  Marriot   TrCovW   TraceW Friedman
## 2 5.1512 43.4620  15.0387 -0.1410  81.1595 690539.9 716.8223 102.8624   6.8084
## 3 2.4059 35.3076   5.6874 -1.0950 124.5851 651900.1 572.6853  78.3233   9.2063
## 4 0.4884 27.6808   2.5302 -2.6596 152.4673 663556.1 455.6235  69.8686  11.1528
##    Rubin Cindex     DB Silhouette   Duda Pseudot2   Beale Ratkowsky    Ball
## 2 1.9055 0.4003 1.0397     0.4085 1.2557  -6.3122 -0.4657    0.4466 51.4312
## 3 2.5024 0.3595 1.1283     0.3094 0.7004   6.8425  0.9679    0.4417 26.1078
## 4 2.8053 0.3525 1.1774     0.2590 1.2645  -3.5562 -0.4629    0.3978 17.4671
##   Ptbiserial    Frey McClain   Dunn Hubert SDindex Dindex   SDbw
## 2     0.6323  1.7441  0.5470 0.2214 0.0085  1.6841 1.3346 1.1098
## 3     0.5414  3.4826  1.1571 0.1194 0.0092  1.8410 1.1499 1.4195
## 4     0.4796 -1.9450  1.6045 0.1235 0.0095  2.1248 1.0671 0.5076
## 
## $All.CriticalValues
##   CritValue_Duda CritValue_PseudoT2 Fvalue_Beale
## 2         0.3890            48.6824       1.0000
## 3         0.3508            29.6102       0.4318
## 4         0.2805            43.6156       1.0000
## 
## $Best.nc
##                     KL     CH Hartigan    CCC   Scott  Marriot  TrCovW  TraceW
## Number_clusters 2.0000  2.000   3.0000  2.000  3.0000     3.00   3.000  3.0000
## Value_Index     5.1512 43.462   9.3513 -0.141 43.4256 50295.68 144.137 16.0844
##                 Friedman   Rubin Cindex     DB Silhouette   Duda PseudoT2
## Number_clusters   3.0000  3.0000 4.0000 2.0000     2.0000 2.0000   2.0000
## Value_Index       2.3979 -0.2942 0.3525 1.0397     0.4085 1.2557  -6.3122
##                   Beale Ratkowsky    Ball PtBiserial   Frey McClain   Dunn
## Number_clusters  2.0000    2.0000  3.0000     2.0000 3.0000   2.000 2.0000
## Value_Index     -0.4657    0.4466 25.3234     0.6323 3.4826   0.547 0.2214
##                 Hubert SDindex Dindex   SDbw
## Number_clusters      0  2.0000      0 4.0000
## Value_Index          0  1.6841      0 0.5076
## 
## $Best.partition
##        Alabama         Alaska        Arizona       Arkansas     California 
##              1              1              1              2              1 
##       Colorado    Connecticut       Delaware        Florida        Georgia 
##              1              2              2              1              1 
##         Hawaii          Idaho       Illinois        Indiana           Iowa 
##              2              2              1              2              2 
##         Kansas       Kentucky      Louisiana          Maine       Maryland 
##              2              2              1              2              1 
##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
##              2              1              2              1              1 
##        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
##              2              2              1              2              2 
##     New Mexico       New York North Carolina   North Dakota           Ohio 
##              1              1              1              2              2 
##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
##              2              2              2              2              1 
##   South Dakota      Tennessee          Texas           Utah        Vermont 
##              2              1              1              2              2 
##       Virginia     Washington  West Virginia      Wisconsin        Wyoming 
##              2              2              2              2              2</code></pre>
<div id="k-means-in-python" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> K-Means in Python<a href="clustering.html#k-means-in-python" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="clustering.html#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="clustering.html#cb21-2" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb21-3"><a href="clustering.html#cb21-3" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb21-4"><a href="clustering.html#cb21-4" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb21-5"><a href="clustering.html#cb21-5" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span>
<span id="cb21-6"><a href="clustering.html#cb21-6" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-7"><a href="clustering.html#cb21-7" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-8"><a href="clustering.html#cb21-8" tabindex="-1"></a></span>
<span id="cb21-9"><a href="clustering.html#cb21-9" tabindex="-1"></a>arrest_py<span class="op">=</span>r.USArrests</span>
<span id="cb21-10"><a href="clustering.html#cb21-10" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb21-11"><a href="clustering.html#cb21-11" tabindex="-1"></a>arrest_scal <span class="op">=</span> scaler.fit_transform(arrest_py)</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():
## C:\PROGRA~3\ANACON~1\lib\site-packages\sklearn\utils\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
##   if not hasattr(array, &quot;sparse&quot;) and array.dtypes.apply(is_sparse).any():</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="clustering.html#cb23-1" tabindex="-1"></a>clus_py<span class="op">=</span>KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">5687</span>, n_init<span class="op">=</span><span class="dv">25</span>).fit(arrest_scal)</span>
<span id="cb23-2"><a href="clustering.html#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="clustering.html#cb23-3" tabindex="-1"></a>clus_py.labels_</span></code></pre></div>
<pre><code>## array([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
##        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
##        0, 0, 0, 0, 0, 0])</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="clustering.html#cb25-1" tabindex="-1"></a></span>
<span id="cb25-2"><a href="clustering.html#cb25-2" tabindex="-1"></a>inertias <span class="op">=</span> []</span>
<span id="cb25-3"><a href="clustering.html#cb25-3" tabindex="-1"></a>silhouette_coefficients <span class="op">=</span> []</span>
<span id="cb25-4"><a href="clustering.html#cb25-4" tabindex="-1"></a>   </span>
<span id="cb25-5"><a href="clustering.html#cb25-5" tabindex="-1"></a>K<span class="op">=</span><span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">10</span>)</span>
<span id="cb25-6"><a href="clustering.html#cb25-6" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> K:</span>
<span id="cb25-7"><a href="clustering.html#cb25-7" tabindex="-1"></a>    kmean1 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k).fit(arrest_scal)</span>
<span id="cb25-8"><a href="clustering.html#cb25-8" tabindex="-1"></a>    kmean1.fit(arrest_scal)</span>
<span id="cb25-9"><a href="clustering.html#cb25-9" tabindex="-1"></a>    inertias.append(kmean1.inertia_)</span>
<span id="cb25-10"><a href="clustering.html#cb25-10" tabindex="-1"></a>    score <span class="op">=</span> silhouette_score(arrest_scal, kmean1.labels_)</span>
<span id="cb25-11"><a href="clustering.html#cb25-11" tabindex="-1"></a>    silhouette_coefficients.append(score)</span></code></pre></div>
<pre><code>## KMeans(n_clusters=2)
## KMeans(n_clusters=3)
## KMeans(n_clusters=4)
## KMeans(n_clusters=5)
## KMeans(n_clusters=6)
## KMeans(n_clusters=7)
## KMeans()
## KMeans(n_clusters=9)</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="clustering.html#cb27-1" tabindex="-1"></a>plt.plot(K, inertias, <span class="st">&#39;bx-&#39;</span>)</span>
<span id="cb27-2"><a href="clustering.html#cb27-2" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Values of K&#39;</span>)</span>
<span id="cb27-3"><a href="clustering.html#cb27-3" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Inertia&#39;</span>)</span>
<span id="cb27-4"><a href="clustering.html#cb27-4" tabindex="-1"></a>plt.title(<span class="st">&#39;The Elbow Method using Inertia&#39;</span>)</span>
<span id="cb27-5"><a href="clustering.html#cb27-5" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="clustering.html#cb28-1" tabindex="-1"></a>plt.plot(K, silhouette_coefficients, <span class="st">&#39;bx-&#39;</span>)</span>
<span id="cb28-2"><a href="clustering.html#cb28-2" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Values of K&#39;</span>)</span>
<span id="cb28-3"><a href="clustering.html#cb28-3" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Silhouette Coefficient&#39;</span>)</span>
<span id="cb28-4"><a href="clustering.html#cb28-4" tabindex="-1"></a>plt.title(<span class="st">&#39;The Silhouette Method&#39;</span>)</span>
<span id="cb28-5"><a href="clustering.html#cb28-5" tabindex="-1"></a>plt.show()</span>
<span id="cb28-6"><a href="clustering.html#cb28-6" tabindex="-1"></a>  </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
</div>
</div>
<div id="hierarchical-clustering" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Hierarchical Clustering<a href="clustering.html#hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In hierarchical clustering, we need to specify the distance metric we wish to use, as well as the linkage information. There are MANY different distance measures, but the two most common are Euclidean and manhattan. We will use the algorithms in the cluster package in R (provides most flexibility in algorithms). In order to do the basic hierarchical clustering, the algorithm is called agnes (Agglomerative “Nested” Hierarchical Clustering). The code below shows how to do this with complete linkage and using Euclidean distances.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="clustering.html#cb29-1" tabindex="-1"></a>dist.assault<span class="ot">=</span><span class="fu">dist</span>(arrest.scal,<span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb29-2"><a href="clustering.html#cb29-2" tabindex="-1"></a>h1.comp.eucl<span class="ot">=</span><span class="fu">agnes</span>(dist.assault,<span class="at">method=</span><span class="st">&quot;complete&quot;</span>)</span>
<span id="cb29-3"><a href="clustering.html#cb29-3" tabindex="-1"></a><span class="fu">pltree</span>(h1.comp.eucl, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">main =</span> <span class="st">&quot;Dendrogram of agnes&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/Hierarchical%20Complete%20Euclidean-5.png" width="672" /></p>
<p>If you want to try a number of different linkage possibilities:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="clustering.html#cb30-1" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="st">&quot;average&quot;</span>, <span class="st">&quot;single&quot;</span>, <span class="st">&quot;complete&quot;</span>, <span class="st">&quot;ward&quot;</span>)</span>
<span id="cb30-2"><a href="clustering.html#cb30-2" tabindex="-1"></a><span class="fu">names</span>(m) <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="st">&quot;average&quot;</span>, <span class="st">&quot;single&quot;</span>, <span class="st">&quot;complete&quot;</span>, <span class="st">&quot;ward&quot;</span>)</span>
<span id="cb30-3"><a href="clustering.html#cb30-3" tabindex="-1"></a></span>
<span id="cb30-4"><a href="clustering.html#cb30-4" tabindex="-1"></a><span class="co"># function to compute coefficient</span></span>
<span id="cb30-5"><a href="clustering.html#cb30-5" tabindex="-1"></a>ac <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb30-6"><a href="clustering.html#cb30-6" tabindex="-1"></a>  <span class="fu">agnes</span>(dist.assault, <span class="at">method =</span> x)<span class="sc">$</span>ac</span>
<span id="cb30-7"><a href="clustering.html#cb30-7" tabindex="-1"></a>}</span>
<span id="cb30-8"><a href="clustering.html#cb30-8" tabindex="-1"></a></span>
<span id="cb30-9"><a href="clustering.html#cb30-9" tabindex="-1"></a><span class="fu">map_dbl</span>(m, ac)</span></code></pre></div>
<pre><code>##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210</code></pre>
<p>In order to get cluster identification, you will need to tell R where you want to cut your dendrogram.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="clustering.html#cb32-1" tabindex="-1"></a>h2<span class="ot">=</span><span class="fu">agnes</span>(dist.assault,<span class="at">method=</span><span class="st">&quot;ward&quot;</span>)</span>
<span id="cb32-2"><a href="clustering.html#cb32-2" tabindex="-1"></a>h2_clus <span class="ot">&lt;-</span> <span class="fu">cutree</span>(h2, <span class="at">k =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>You can also compare different dendrograms….</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="clustering.html#cb33-1" tabindex="-1"></a>dend1 <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span> (h1.comp.eucl)</span>
<span id="cb33-2"><a href="clustering.html#cb33-2" tabindex="-1"></a>dend2 <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span> (h2)</span>
<span id="cb33-3"><a href="clustering.html#cb33-3" tabindex="-1"></a></span>
<span id="cb33-4"><a href="clustering.html#cb33-4" tabindex="-1"></a><span class="fu">tanglegram</span>(dend1, dend2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/compare%20dendrograms-1.png" width="672" /></p>
<p>You can also use the measures from the NbClust algorithm here, too….</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="clustering.html#cb34-1" tabindex="-1"></a><span class="fu">NbClust</span>(arrest.scal,<span class="at">distance=</span><span class="st">&quot;euclidean&quot;</span>,<span class="at">method=</span><span class="st">&quot;complete&quot;</span>,<span class="at">min.nc=</span><span class="dv">2</span>,<span class="at">max.nc =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/NbClust%20with%20HC-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="bookdownproj_files/figure-html/NbClust%20with%20HC-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 9 proposed 2 as the best number of clusters 
## * 9 proposed 3 as the best number of clusters 
## * 5 proposed 4 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************</code></pre>
<pre><code>## $All.index
##         KL      CH Hartigan     CCC    Scott  Marriot   TrCovW   TraceW
## 2  26.3587 41.8949  11.5200 -0.3483  78.6988 725375.1 708.1600 104.6556
## 3   0.0704 31.0737  18.9426 -2.0049 107.4372 918599.5 331.2367  84.3997
## 4 393.5832 34.6264   5.4577 -0.7050 156.0842 617250.0 257.9307  60.1552
##   Friedman  Rubin Cindex     DB Silhouette   Duda Pseudot2   Beale Ratkowsky
## 2   6.7099 1.8728 0.4031 1.0513     0.4048 0.5525  13.7698  1.8468    0.4404
## 3   8.1357 2.3223 0.3800 1.0433     0.3692 0.5918  20.0040  1.6098    0.4305
## 4  10.8610 3.2582 0.4334 1.0811     0.3160 1.0527  -0.3001 -0.1035    0.4155
##      Ball Ptbiserial   Frey McClain   Dunn Hubert SDindex Dindex   SDbw
## 2 52.3278     0.6330 0.9663  0.5287 0.2637 0.0087  1.3745 1.3451 1.1226
## 3 28.1332     0.6315 0.8670  0.6873 0.2648 0.0092  1.2895 1.2025 0.7789
## 4 15.0388     0.5820 0.0448  1.3366 0.1622 0.0108  1.4085 1.0199 0.5029
## 
## $All.CriticalValues
##   CritValue_Duda CritValue_PseudoT2 Fvalue_Beale
## 2         0.3773            28.0561       0.1300
## 3         0.4780            31.6758       0.1765
## 4         0.1265            41.4361       1.0000
## 
## $Best.nc
##                       KL      CH Hartigan     CCC  Scott Marriot   TrCovW
## Number_clusters   4.0000  2.0000   4.0000  2.0000  4.000       3   3.0000
## Value_Index     393.5832 41.8949  13.4849 -0.3483 48.647 -494574 376.9233
##                  TraceW Friedman  Rubin Cindex     DB Silhouette   Duda
## Number_clusters  3.0000   4.0000 3.0000   3.00 3.0000     2.0000 2.0000
## Value_Index     -3.9886   2.7253 0.4865   0.38 1.0433     0.4048 0.5525
##                 PseudoT2  Beale Ratkowsky    Ball PtBiserial Frey McClain
## Number_clusters   2.0000 2.0000    2.0000  3.0000      2.000    1  2.0000
## Value_Index      13.7698 1.8468    0.4404 24.1946      0.633   NA  0.5287
##                   Dunn Hubert SDindex Dindex   SDbw
## Number_clusters 3.0000      0  3.0000      0 4.0000
## Value_Index     0.2648      0  1.2895      0 0.5029
## 
## $Best.partition
##        Alabama         Alaska        Arizona       Arkansas     California 
##              1              1              1              2              1 
##       Colorado    Connecticut       Delaware        Florida        Georgia 
##              1              2              2              1              1 
##         Hawaii          Idaho       Illinois        Indiana           Iowa 
##              2              2              1              2              2 
##         Kansas       Kentucky      Louisiana          Maine       Maryland 
##              2              2              1              2              1 
##  Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
##              2              1              2              1              2 
##        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
##              2              2              1              2              2 
##     New Mexico       New York North Carolina   North Dakota           Ohio 
##              1              1              1              2              2 
##       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
##              2              2              2              2              1 
##   South Dakota      Tennessee          Texas           Utah        Vermont 
##              2              1              1              2              2 
##       Virginia     Washington  West Virginia      Wisconsin        Wyoming 
##              2              2              2              2              2</code></pre>
<div id="python-for-hierarchical-clustering" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Python for Hierarchical Clustering<a href="clustering.html#python-for-hierarchical-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To view the dendogram in python, you can use the linkage command (caution!!! If you have a lot of data points, this is going to get really messy….in other words, I would not recommend it). This data set only has 50 observations, so it is not too bad to view. The method is the type of linkage and metric is the distance metric used.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="clustering.html#cb38-1" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage</span>
<span id="cb38-2"><a href="clustering.html#cb38-2" tabindex="-1"></a></span>
<span id="cb38-3"><a href="clustering.html#cb38-3" tabindex="-1"></a>linkage_data <span class="op">=</span> linkage(arrest_scal, method<span class="op">=</span><span class="st">&#39;ward&#39;</span>, metric<span class="op">=</span><span class="st">&#39;euclidean&#39;</span>)</span>
<span id="cb38-4"><a href="clustering.html#cb38-4" tabindex="-1"></a>dendrogram(linkage_data)</span></code></pre></div>
<pre><code>## {&#39;icoord&#39;: [[5.0, 5.0, 15.0, 15.0], [25.0, 25.0, 35.0, 35.0], [10.0, 10.0, 30.0, 30.0], [55.0, 55.0, 65.0, 65.0], [45.0, 45.0, 60.0, 60.0], [20.0, 20.0, 52.5, 52.5], [85.0, 85.0, 95.0, 95.0], [75.0, 75.0, 90.0, 90.0], [135.0, 135.0, 145.0, 145.0], [125.0, 125.0, 140.0, 140.0], [115.0, 115.0, 132.5, 132.5], [105.0, 105.0, 123.75, 123.75], [82.5, 82.5, 114.375, 114.375], [175.0, 175.0, 185.0, 185.0], [165.0, 165.0, 180.0, 180.0], [155.0, 155.0, 172.5, 172.5], [98.4375, 98.4375, 163.75, 163.75], [36.25, 36.25, 131.09375, 131.09375], [195.0, 195.0, 205.0, 205.0], [215.0, 215.0, 225.0, 225.0], [200.0, 200.0, 220.0, 220.0], [245.0, 245.0, 255.0, 255.0], [235.0, 235.0, 250.0, 250.0], [265.0, 265.0, 275.0, 275.0], [295.0, 295.0, 305.0, 305.0], [285.0, 285.0, 300.0, 300.0], [270.0, 270.0, 292.5, 292.5], [242.5, 242.5, 281.25, 281.25], [210.0, 210.0, 261.875, 261.875], [315.0, 315.0, 325.0, 325.0], [335.0, 335.0, 345.0, 345.0], [320.0, 320.0, 340.0, 340.0], [365.0, 365.0, 375.0, 375.0], [385.0, 385.0, 395.0, 395.0], [370.0, 370.0, 390.0, 390.0], [355.0, 355.0, 380.0, 380.0], [415.0, 415.0, 425.0, 425.0], [405.0, 405.0, 420.0, 420.0], [445.0, 445.0, 455.0, 455.0], [435.0, 435.0, 450.0, 450.0], [465.0, 465.0, 475.0, 475.0], [442.5, 442.5, 470.0, 470.0], [485.0, 485.0, 495.0, 495.0], [456.25, 456.25, 490.0, 490.0], [412.5, 412.5, 473.125, 473.125], [367.5, 367.5, 442.8125, 442.8125], [330.0, 330.0, 405.15625, 405.15625], [235.9375, 235.9375, 367.578125, 367.578125], [83.671875, 83.671875, 301.7578125, 301.7578125]], &#39;dcoord&#39;: [[0.0, 0.7800624726117171, 0.7800624726117171, 0.0], [0.0, 1.0225018692038894, 1.0225018692038894, 0.0], [0.7800624726117171, 1.1021694044411907, 1.1021694044411907, 1.0225018692038894], [0.0, 0.7945530103578827, 0.7945530103578827, 0.0], [0.0, 1.2197191288489164, 1.2197191288489164, 0.7945530103578827], [1.1021694044411907, 2.7421139544450304, 2.7421139544450304, 1.2197191288489164], [0.0, 0.3537743681310408, 0.3537743681310408, 0.0], [0.0, 0.952332839989021, 0.952332839989021, 0.3537743681310408], [0.0, 0.5408248189680332, 0.5408248189680332, 0.0], [0.0, 0.9610307025238368, 0.9610307025238368, 0.5408248189680332], [0.0, 1.3122402649731468, 1.3122402649731468, 0.9610307025238368], [0.0, 1.7079251328916916, 1.7079251328916916, 1.3122402649731468], [0.952332839989021, 2.2649420381778995, 2.2649420381778995, 1.7079251328916916], [0.0, 1.2089769072372043, 1.2089769072372043, 0.0], [0.0, 1.404214498699647, 1.404214498699647, 1.2089769072372043], [0.0, 3.055735303919404, 3.055735303919404, 1.404214498699647], [2.2649420381778995, 3.531541898650026, 3.531541898650026, 3.055735303919404], [2.7421139544450304, 6.527470828535695, 6.527470828535695, 3.531541898650026], [0.0, 0.7180984285039761, 0.7180984285039761, 0.0], [0.0, 0.9924604112760993, 0.9924604112760993, 0.0], [0.7180984285039761, 1.3303363086847781, 1.3303363086847781, 0.9924604112760993], [0.0, 0.7464962542035871, 0.7464962542035871, 0.0], [0.0, 0.815842993431543, 0.815842993431543, 0.7464962542035871], [0.0, 0.4990993894073977, 0.4990993894073977, 0.0], [0.0, 0.2079437976133826, 0.2079437976133826, 0.0], [0.0, 0.6625852157172658, 0.6625852157172658, 0.2079437976133826], [0.4990993894073977, 1.3585660787207545, 1.3585660787207545, 0.6625852157172658], [0.815842993431543, 1.7661673389439376, 1.7661673389439376, 1.3585660787207545], [1.3303363086847781, 3.023678727416965, 3.023678727416965, 1.7661673389439376], [0.0, 0.7109765828248565, 0.7109765828248565, 0.0], [0.0, 1.0705701702968808, 1.0705701702968808, 0.0], [0.7109765828248565, 1.1724104281482475, 1.1724104281482475, 1.0705701702968808], [0.0, 0.8058634904273213, 0.8058634904273213, 0.0], [0.0, 1.0865316609298954, 1.0865316609298954, 0.0], [0.8058634904273213, 1.55526892227882, 1.55526892227882, 1.0865316609298954], [0.0, 1.8302005688424474, 1.8302005688424474, 1.55526892227882], [0.0, 0.59956022650398, 0.59956022650398, 0.0], [0.0, 1.2729224911920178, 1.2729224911920178, 0.59956022650398], [0.0, 0.43312429085085896, 0.43312429085085896, 0.0], [0.0, 0.5591483670409593, 0.5591483670409593, 0.43312429085085896], [0.0, 0.7860298248284557, 0.7860298248284557, 0.0], [0.5591483670409593, 0.8558054806051812, 0.8558054806051812, 0.7860298248284557], [0.0, 1.0818450636699195, 1.0818450636699195, 0.0], [0.8558054806051812, 1.95573383070349, 1.95573383070349, 1.0818450636699195], [1.2729224911920178, 2.225708599114937, 2.225708599114937, 1.95573383070349], [1.8302005688424474, 3.2433103740861906, 3.2433103740861906, 2.225708599114937], [1.1724104281482475, 3.772026252105819, 3.772026252105819, 3.2433103740861906], [3.023678727416965, 7.261167758993684, 7.261167758993684, 3.772026252105819], [6.527470828535695, 13.653466603337856, 13.653466603337856, 7.261167758993684]], &#39;ivl&#39;: [&#39;0&#39;, &#39;17&#39;, &#39;9&#39;, &#39;41&#39;, &#39;32&#39;, &#39;23&#39;, &#39;39&#39;, &#39;42&#39;, &#39;12&#39;, &#39;31&#39;, &#39;8&#39;, &#39;2&#39;, &#39;21&#39;, &#39;19&#39;, &#39;30&#39;, &#39;1&#39;, &#39;5&#39;, &#39;4&#39;, &#39;27&#39;, &#39;40&#39;, &#39;47&#39;, &#39;33&#39;, &#39;44&#39;, &#39;11&#39;, &#39;25&#39;, &#39;26&#39;, &#39;22&#39;, &#39;48&#39;, &#39;18&#39;, &#39;14&#39;, &#39;28&#39;, &#39;45&#39;, &#39;49&#39;, &#39;3&#39;, &#39;16&#39;, &#39;7&#39;, &#39;20&#39;, &#39;29&#39;, &#39;6&#39;, &#39;38&#39;, &#39;24&#39;, &#39;36&#39;, &#39;46&#39;, &#39;35&#39;, &#39;13&#39;, &#39;15&#39;, &#39;34&#39;, &#39;37&#39;, &#39;10&#39;, &#39;43&#39;], &#39;leaves&#39;: [0, 17, 9, 41, 32, 23, 39, 42, 12, 31, 8, 2, 21, 19, 30, 1, 5, 4, 27, 40, 47, 33, 44, 11, 25, 26, 22, 48, 18, 14, 28, 45, 49, 3, 16, 7, 20, 29, 6, 38, 24, 36, 46, 35, 13, 15, 34, 37, 10, 43], &#39;color_list&#39;: [&#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C0&#39;], &#39;leaves_color_list&#39;: [&#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;]}</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="clustering.html#cb40-1" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>To do Agglomerative Clustering:</p>
<p>Note that you need to specify the number of clusters here. You are able to say None for the number of clusters, but then you need to specify the distance threshold (from dendogram). Again illustrating that this is not the best method for large data sets.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="clustering.html#cb41-1" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb41-2"><a href="clustering.html#cb41-2" tabindex="-1"></a></span>
<span id="cb41-3"><a href="clustering.html#cb41-3" tabindex="-1"></a>aggclus_py <span class="op">=</span> AgglomerativeClustering(n_clusters <span class="op">=</span> <span class="dv">2</span>, affinity <span class="op">=</span> <span class="st">&#39;euclidean&#39;</span>, linkage <span class="op">=</span> <span class="st">&#39;complete&#39;</span>)</span>
<span id="cb41-4"><a href="clustering.html#cb41-4" tabindex="-1"></a>arrest_hy <span class="op">=</span> aggclus_py.fit_predict(arrest_scal)</span></code></pre></div>
<p>Note: I did not find a good package for divisive clustering in Python.</p>
<p>##DBSCAN</p>
<p>DBSCAN is one of the most popular density-based clustering algorithm in use. Density Based Spatial Clustering Applications with Noise (DBSCAN) cluster points together that are “density reachable” (within a certain radius of each other). Users can specify this distance and the minimum number of points that will be used to create clusters (see <a href="https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf" class="uri">https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf</a>)</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="clustering.html#cb42-1" tabindex="-1"></a>scan1<span class="ot">&lt;-</span><span class="fu">hdbscan</span>(arrest.scal,<span class="at">minPts=</span><span class="dv">4</span>)</span>
<span id="cb42-2"><a href="clustering.html#cb42-2" tabindex="-1"></a>pca_ex<span class="ot">=</span><span class="fu">prcomp</span>(arrest.scal,<span class="at">scale=</span>F)</span>
<span id="cb42-3"><a href="clustering.html#cb42-3" tabindex="-1"></a>scan1data<span class="ot">=</span><span class="fu">cbind.data.frame</span>(pca_ex<span class="sc">$</span>x[,<span class="dv">1</span>],pca_ex<span class="sc">$</span>x[,<span class="dv">2</span>],<span class="fu">as.factor</span>(scan1<span class="sc">$</span>cluster<span class="sc">+</span><span class="dv">1</span>))</span>
<span id="cb42-4"><a href="clustering.html#cb42-4" tabindex="-1"></a><span class="fu">colnames</span>(scan1data)<span class="ot">=</span><span class="fu">c</span>(<span class="st">&quot;PCA1&quot;</span>,<span class="st">&quot;PCA2&quot;</span>,<span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb42-5"><a href="clustering.html#cb42-5" tabindex="-1"></a><span class="fu">ggplot</span>(scan1data,<span class="fu">aes</span>(<span class="at">x=</span>PCA1,<span class="at">y=</span>PCA2,<span class="at">color=</span>cluster))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span> <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Dark2&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/hdbscan-3.png" width="672" /></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="clustering.html#cb43-1" tabindex="-1"></a><span class="fu">plot</span>(scan1,<span class="at">show_flat=</span>T)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/hdbscan-4.png" width="672" /></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="clustering.html#cb44-1" tabindex="-1"></a><span class="co">#res.dbscan=dbscan(arrest.scal,eps=1.2,minPts=4)</span></span>
<span id="cb44-2"><a href="clustering.html#cb44-2" tabindex="-1"></a>d<span class="ot">=</span><span class="fu">dist</span>(arrest.scal,<span class="at">method =</span> <span class="st">&quot;canberra&quot;</span>)</span>
<span id="cb44-3"><a href="clustering.html#cb44-3" tabindex="-1"></a>res.dbscan<span class="ot">=</span><span class="fu">dbscan</span>(d,<span class="at">eps=</span><span class="fl">1.2</span>,<span class="at">minPts=</span><span class="dv">4</span>)</span>
<span id="cb44-4"><a href="clustering.html#cb44-4" tabindex="-1"></a>res.dbscan</span></code></pre></div>
<pre><code>## DBSCAN clustering for 50 objects.
## Parameters: eps = 1.2, minPts = 4
## Using canberra distances and borderpoints = TRUE
## The clustering contains 2 cluster(s) and 27 noise points.
## 
##  0  1  2 
## 27 10 13 
## 
## Available fields: cluster, eps, minPts, dist, borderPoints</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="clustering.html#cb46-1" tabindex="-1"></a>scan1data<span class="ot">=</span><span class="fu">cbind.data.frame</span>(pca_ex<span class="sc">$</span>x[,<span class="dv">1</span>],pca_ex<span class="sc">$</span>x[,<span class="dv">2</span>],<span class="fu">as.factor</span>(res.dbscan<span class="sc">$</span>cluster<span class="sc">+</span><span class="dv">1</span>))</span>
<span id="cb46-2"><a href="clustering.html#cb46-2" tabindex="-1"></a><span class="fu">colnames</span>(scan1data)<span class="ot">=</span><span class="fu">c</span>(<span class="st">&quot;PCA1&quot;</span>,<span class="st">&quot;PCA2&quot;</span>,<span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb46-3"><a href="clustering.html#cb46-3" tabindex="-1"></a><span class="fu">ggplot</span>(scan1data,<span class="fu">aes</span>(<span class="at">x=</span>PCA1,<span class="at">y=</span>PCA2,<span class="at">color=</span>cluster))<span class="sc">+</span><span class="fu">geom_point</span>()<span class="sc">+</span> <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Dark2&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/hdbscan-5.png" width="672" /></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="clustering.html#cb47-1" tabindex="-1"></a><span class="do">###Nice visuals for PCA results:</span></span>
<span id="cb47-2"><a href="clustering.html#cb47-2" tabindex="-1"></a><span class="do">## http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/</span></span></code></pre></div>
</div>
</div>
<div id="variable-clustering" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Variable Clustering<a href="clustering.html#variable-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="clustering.html#cb48-1" tabindex="-1"></a>telco<span class="ot">=</span><span class="fu">read.csv</span>(<span class="st">&quot;Q:</span><span class="sc">\\</span><span class="st">My Drive</span><span class="sc">\\</span><span class="st">Data Mining</span><span class="sc">\\</span><span class="st">Data</span><span class="sc">\\</span><span class="st">TelcoChurn.csv&quot;</span>,<span class="at">header=</span>T)</span>
<span id="cb48-2"><a href="clustering.html#cb48-2" tabindex="-1"></a>telco[<span class="fu">is.na</span>(telco)] <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb48-3"><a href="clustering.html#cb48-3" tabindex="-1"></a>quant.var<span class="ot">=</span>telco[,<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">19</span>,<span class="dv">20</span>)]</span>
<span id="cb48-4"><a href="clustering.html#cb48-4" tabindex="-1"></a>qual.var<span class="ot">=</span>telco[,<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">7</span><span class="sc">:</span><span class="dv">18</span>)]</span>
<span id="cb48-5"><a href="clustering.html#cb48-5" tabindex="-1"></a></span>
<span id="cb48-6"><a href="clustering.html#cb48-6" tabindex="-1"></a>var.clust.h<span class="ot">=</span><span class="fu">hclustvar</span>(quant.var, qual.var)</span>
<span id="cb48-7"><a href="clustering.html#cb48-7" tabindex="-1"></a>stab<span class="ot">=</span><span class="fu">stability</span>(var.clust.h,<span class="at">B=</span><span class="dv">50</span>) <span class="do">## This will take time!</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/variable%20clustering-1.png" width="672" /></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="clustering.html#cb49-1" tabindex="-1"></a><span class="fu">plot</span>(stab)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/variable%20clustering-2.png" width="672" /></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="clustering.html#cb50-1" tabindex="-1"></a>h6<span class="ot">=</span><span class="fu">cutreevar</span>(var.clust.h, <span class="dv">6</span>)</span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="other-topics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/sjsimmo2/DataMining-Fall/edit/master/03-Clustering.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/sjsimmo2/DataMining-Fall/blob/master/03-Clustering.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
