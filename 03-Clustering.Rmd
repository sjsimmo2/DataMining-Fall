

```{r library,include=FALSE}
library(datasets)
library(arules)
library(arulesViz)
library(ggplot2)
library(dplyr)
library(rpart)
library(rpart.plot)
library(TH.data)
library(ISLR2)
library(lattice)
library(stats)
library(rattle)
library(RColorBrewer)
library(caret)
library(ROCR)
library(tidyverse)  
library(cluster)  
library(factoextra) 
library(gridExtra)
library(NbClust)
library(dendextend)
library(class)
library(ClustOfVar)
library(MASS)
library(partykit)
library(dbscan)


check=read.table("Q:\\My Drive\\Data Mining\\Data\\Checking.csv",sep=',',header=T)

temp.dat=read.table("Q:\\My Drive\\Data Mining\\Data\\Grocery1.csv",sep=',',header=T)

load("Q:\\My Drive\\Data Mining\\Data\\breast_cancer.Rdata")

library(reticulate)

use_python("C:\\ProgramData\\Anaconda3\\python.exe")

```


# Clustering

We will now get into clustering.  There are two broad areas of clustering: hard and soft.  Hard clustering is when an observation can belong to one and only one cluster.  Soft clustering is when an observation receives some type of measure (for example, a probability) of being in a cluster.  We will be focusing on hard clustering. 

Within hard clustering, algorithms can be flat (for example, kmeans) or hierarchical.  Finally, within hierarchical, we have agglomeratie and divisive (just like they sound...agglomerative means we will build things up where each observations starts as its own cluster; divisive means we will tear things down which means all observations start in one cluster).

We will start by looking at kmeans, then move into hierarchical clustering.

## Kmeans

We will be using the USArrests data set (this dat is already in R). It is an older data set, but still good to illustrate clustering.  This data has 50 observations (one observation per state) and contains statistics in arrests per 100,000 residents for assault, murder and rape.  Also provided is the percent of the population in that state who live in urban areas.

```{r cluster preprocess}
summary(USArrests)
hist(USArrests$Murder)
hist(USArrests$Assault)
hist(USArrests$Rape)
hist(USArrests$UrbanPop)
arrest.scal=scale(USArrests)

```

With only 4 variables and good scales, we will just use the scaled data (arrest.scal) for the clustering.

```{r kmeans}

clus2=kmeans(arrest.scal,centers=2,nstart = 25)
clus2
fviz_cluster(clus2, data = arrest.scal)

```

We do not know if 2 clusters is the best number of clusters, so we will need to explore other options and determine which is the best number of clusters for this data set.

```{r numofclusters}

set.seed(12964)

fviz_nbclust(arrest.scal, kmeans, method = "wss",k.max = 9)

fviz_nbclust(arrest.scal, kmeans, method = "silhouette",k.max = 9)

set.seed(123)
gap_stat = clusGap(arrest.scal, FUN = kmeans, nstart = 25, K.max = 9, B = 50)
fviz_gap_stat(gap_stat)

```

As you can see, different statistics provide different number of clusters (of course!).  As a final thought, let's plot all of these using PCA.
```{r visualize clusters}

k2 <- kmeans(arrest.scal, centers = 2, nstart = 25)
k3 <- kmeans(arrest.scal, centers = 3, nstart = 25)
k4 <- kmeans(arrest.scal, centers = 4, nstart = 25)

# plots to compare
p2 <- fviz_cluster(k2, geom = "point", data = arrest.scal) + ggtitle("k = 2")
p3 <- fviz_cluster(k3, geom = "point",  data = arrest.scal) + ggtitle("k = 3")
p4 <- fviz_cluster(k4, geom = "point",  data = arrest.scal) + ggtitle("k = 4")


grid.arrange(p2, p3, p4, nrow = 2)


```

Going with 2 clusters, we can now try to profile the data. First, we need to merge the data with the cluster numbers.

```{r kmeans profile}

profile.kmeans=cbind(USArrests,k2$cluster)
all.k=profile.kmeans %>% group_by(k2$cluster) %>%
   summarise(mean.assault=mean(Assault),mean.murder=mean(Murder),mean.rape=mean(Rape),mean.pop=mean(UrbanPop))
 all.k

 NbClust(arrest.scal,method="kmeans",min.nc=2,max.nc = 4)

```

### K-Means in Python

```{python}

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cdist
import numpy as np
import matplotlib.pyplot as plt

arrest_py=r.USArrests
scaler = StandardScaler()
arrest_scal = scaler.fit_transform(arrest_py)
clus_py=KMeans(n_clusters=2, random_state=5687, n_init=25).fit(arrest_scal)

clus_py.labels_
```

```{python}

inertias = []
silhouette_coefficients = []
   
K=range(2,10)
for k in K:
    kmean1 = KMeans(n_clusters=k).fit(arrest_scal)
    kmean1.fit(arrest_scal)
    inertias.append(kmean1.inertia_)
    score = silhouette_score(arrest_scal, kmean1.labels_)
    silhouette_coefficients.append(score)

plt.plot(K, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

plt.plot(K, silhouette_coefficients, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Silhouette Coefficient')
plt.title('The Silhouette Method')
plt.show()
  
```



## Hierarchical Clustering

In hierarchical clustering, we need to specify the distance metric we wish to use, as well as the linkage information.  There are MANY different distance measures, but the two most common are Euclidean and manhattan.  We will use the algorithms in the cluster package in R (provides most flexibility in algorithms).  In order to do the basic hierarchical clustering, the algorithm is called agnes (Agglomerative "Nested" Hierarchical Clustering).  The code below shows how to do this with complete linkage and using Euclidean distances.

```{r Hierarchical Complete Euclidean}

dist.assault=dist(arrest.scal,method = "euclidean")
h1.comp.eucl=agnes(dist.assault,method="complete")
pltree(h1.comp.eucl, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
 
```

If you want to try a number of different linkage possibilities:

```{r various agglomerative approaches}


m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dist.assault, method = x)$ac
}

map_dbl(m, ac)
```

In order to get cluster identification, you will need to tell R where you want to cut your dendrogram.

```{r create clusters}
h2=agnes(dist.assault,method="ward")
h2_clus <- cutree(h2, k = 2)

```

You can also compare different dendrograms....

```{r compare dendrograms}

dend1 <- as.dendrogram (h1.comp.eucl)
dend2 <- as.dendrogram (h2)

tanglegram(dend1, dend2)

```

You can also use the measures from the NbClust algorithm here, too....

```{r NbClust with HC}

NbClust(arrest.scal,distance="euclidean",method="complete",min.nc=2,max.nc = 4)

```


### Python for Hierarchical Clustering

To view the dendogram in python, you can use the linkage command (caution!!! If you have a lot of data points, this is going to get really messy....in other words, I would not recommend it).  This data set only has 50 observations, so it is not too bad to view.  The method is the type of linkage and metric is the distance metric used.

```{python}
from scipy.cluster.hierarchy import dendrogram, linkage

linkage_data = linkage(arrest_scal, method='ward', metric='euclidean')
dendrogram(linkage_data)

plt.show()
```


To do Agglomerative Clustering:  

Note that you need to specify the number of clusters here.  You are able to say None for the number of clusters, but then you need to specify the distance threshold (from dendogram).  Again illustrating that this is not the best method for large data sets. 

```{python}
from sklearn.cluster import AgglomerativeClustering

aggclus_py = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'complete')
arrest_hy = aggclus_py.fit_predict(arrest_scal)


```

I did not find a good package for divisive clustering in Python.


## DBSCAN

DBSCAN is one of the most popular density-based clustering algorithm in use. Density Based Spatial Clustering Applications with Noise (DBSCAN) cluster points together that are "density reachable" (within a certain radius of each other). Users can specify this distance and the minimum number of points that will be used to create clusters (see https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf)

```{r hdbscan}

scan1<-hdbscan(arrest.scal,minPts=4)
pca_ex=prcomp(arrest.scal,scale=F)
scan1data=cbind.data.frame(pca_ex$x[,1],pca_ex$x[,2],as.factor(scan1$cluster+1))
colnames(scan1data)=c("PCA1","PCA2","cluster")
ggplot(scan1data,aes(x=PCA1,y=PCA2,color=cluster))+geom_point()+ scale_fill_brewer(palette = "Dark2")

plot(scan1,show_flat=T)

#res.dbscan=dbscan(arrest.scal,eps=1.2,minPts=4)
d=dist(arrest.scal,method = "canberra")
res.dbscan=dbscan(d,eps=1.2,minPts=4)
res.dbscan

scan1data=cbind.data.frame(pca_ex$x[,1],pca_ex$x[,2],as.factor(res.dbscan$cluster+1))
colnames(scan1data)=c("PCA1","PCA2","cluster")
ggplot(scan1data,aes(x=PCA1,y=PCA2,color=cluster))+geom_point()+ scale_fill_brewer(palette = "Dark2")


###Nice visuals for PCA results:
## http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
```


### DBSCAN in Python

```{python}
from sklearn.cluster import DBSCAN
from collections import Counter


db_py = DBSCAN(eps=1.2,min_samples=4).fit(arrest_scal)

db_py.labels_ 

set(db_py.labels_)
Counter(db_py.labels_)

```




## Variable Clustering

```{r variable clustering}
telco=read.csv("Q:\\My Drive\\Data Mining\\Data\\TelcoChurn.csv",header=T)
telco[is.na(telco)] = 0
quant.var=telco[,c(3,6,19,20)]
qual.var=telco[,c(2,4,5,7:18)]

var.clust.h=hclustvar(quant.var, qual.var)
stab=stability(var.clust.h,B=50) ## This will take time!
plot(stab)

h6=cutreevar(var.clust.h, 6)
```

